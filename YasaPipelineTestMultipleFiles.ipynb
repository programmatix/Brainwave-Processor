{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Autoreload possibly interferes with IntelliJ debugging\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "log = lambda msg: logging.info(msg)\n"
   ],
   "id": "34b2ec6a149ab9b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "f89ac5fec644dc7a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full night pipeline (multiple files)",
   "id": "2177b708d94480ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_dir = \"C:\\\\dev\\\\play\\\\brainwave-data\"\n",
    "stats_df = pd.read_csv(input_dir + os.path.sep + \"stats.csv\")"
   ],
   "id": "4b29301dac33b36d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Find Brainflow files that haven't been copied over",
   "id": "a68b61c44f8dcdc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import paramiko\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Define the time window\n",
    "time_window = timedelta(minutes=10)\n",
    "\n",
    "# Define the remote server details\n",
    "hostname = os.getenv('SSH_HOST')\n",
    "username = os.getenv('SSH_USERNAME')\n",
    "password = os.getenv('SSH_PASSWORD')\n",
    "remote_dir = '/home/graham/dev/Brainwave-Data'\n",
    "local_dir = input_dir\n",
    "\n",
    "# Create an SSH client\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(hostname, username=username, password=password, compress=True)\n",
    "\n",
    "# List files in the remote directory\n",
    "stdin, stdout, stderr = ssh.exec_command(f'ls {remote_dir}')\n",
    "remote_brainflow_files = [f for f in stdout.read().decode().splitlines() if f.endswith(\".brainflow.csv\")]\n"
   ],
   "id": "de7c66d190ca587d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "remote_brainflow_files",
   "id": "575825f9db239c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files_to_copy = remote_brainflow_files.copy()\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    #for idx, file_name in enumerate(tqdm(dirs, desc=\"Converting Brainflow to FIF\")):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if \".brainflow.csv.bz2\" in full_input_filename:\n",
    "            file_name_without_bz2 = file_name.removesuffix(\".bz2\")\n",
    "            already_have = file_name_without_bz2 in remote_brainflow_files\n",
    "            print(f\"Already have {full_input_filename}: {already_have}\")\n",
    "            if already_have:\n",
    "                files_to_copy.remove(file_name_without_bz2)\n",
    "\n"
   ],
   "id": "5cb4213ad55fa5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "files_to_copy",
   "id": "423cd6c357456f29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import bz2\n",
    "from tqdm import tqdm\n",
    "\n",
    "sftp = ssh.open_sftp()\n",
    "for file in files_to_copy:\n",
    "    remote_file_path = remote_dir + \"/\" + file\n",
    "    local_file_path = os.path.join(input_dir, file)\n",
    "    log(f\"Copying Brainflow backup remote:{remote_file_path} to {local_file_path}\")\n",
    "\n",
    "    # Get the file size\n",
    "    remote_file_size = sftp.stat(remote_file_path).st_size\n",
    "\n",
    "    with tqdm(total=remote_file_size, unit='B', unit_scale=True, desc=file, ascii=True) as pbar:\n",
    "        local_file_path = os.path.join(input_dir, file)\n",
    "    \n",
    "        def callback(transferred_so_far, total_to_transfer):\n",
    "            pbar.update(transferred_so_far - pbar.n)\n",
    "    \n",
    "        # Create a file-like object that writes to the local file\n",
    "        with open(local_file_path, 'wb') as local_file:\n",
    "            sftp.getfo(remote_file_path, local_file, callback=callback)\n",
    "sftp.close()"
   ],
   "id": "cb454f01364aee73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ssh.close()\n",
   "id": "42674aa9df2a9b98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert Brainflow files to FIF",
   "id": "6558e15223cc8efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_brainflow_compressed_filename(full_input_filename: str) -> str:\n",
    "    full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "    compressed_full_output_filename = str(os.path.join(full_output_dirname, os.path.basename(full_input_filename))) + '.bz2'\n",
    "    return compressed_full_output_filename"
   ],
   "id": "e93d35cf861b09fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import webserver\n",
    "import convert\n",
    "# import zstandard as zstd\n",
    "import os\n",
    "import bz2\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "errors = []\n",
    "processed = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = ['2024-09-10-21-22-21']\n",
    "\n",
    "def compress_bz2(input_file, output_file):\n",
    "    start_time = time.time()\n",
    "    with open(input_file, 'rb') as f_in:\n",
    "        with bz2.open(output_file, 'wb', compresslevel=9) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, os.path.getsize(output_file)\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    #for idx, file_name in enumerate(tqdm(dirs, desc=\"Converting Brainflow to FIF\")):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        try:\n",
    "            if full_input_filename.endswith(\".brainflow.csv\"):\n",
    "                full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "                full_output_filename = str(os.path.join(full_output_dirname, 'raw.fif'))\n",
    "                \n",
    "                compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "                \n",
    "                if not os.path.exists(compressed_full_output_filename):\n",
    "                    log(f\"Compressing file {full_input_filename} to \" + compressed_full_output_filename)\n",
    "                    processed.append(\"Compressing \" + full_input_filename)\n",
    "                    try:\n",
    "                        os.mkdir(os.path.dirname(compressed_full_output_filename))\n",
    "                    except:\n",
    "                        pass\n",
    "                    compress_bz2(full_input_filename, compressed_full_output_filename) \n",
    "                    \n",
    "                if os.path.exists(full_output_filename):\n",
    "                    log(f\"Skipping file {full_input_filename} as {full_output_filename} and {compressed_full_output_filename} already exist\")\n",
    "                    continue\n",
    "                should_skip = False\n",
    "                for s in skip_list:\n",
    "                    if s in full_input_filename:\n",
    "                        log(f\"Skipping file {full_input_filename}\")\n",
    "                        should_skip = True\n",
    "                if not should_skip:\n",
    "                    log(f\"Processing file {full_input_filename}\")\n",
    "                    processed.append(\"Processing \" + full_input_filename)\n",
    "                    channels = None\n",
    "                    date_time_str = os.path.basename(full_input_filename).removesuffix(\".brainflow.csv\")\n",
    "                    date_time_obj = datetime.strptime(date_time_str, '%Y-%m-%d-%H-%M-%S')\n",
    "                    if (date_time_obj > datetime(2024, 9, 1, 0, 0, 0)):\n",
    "                        channels = ['Fpz-M1']\n",
    "        \n",
    "                    if channels is not None:\n",
    "                        log(f\"Processing file {full_input_filename} with channels {channels}\")\n",
    "                        # Changing to gap-filling on 30th Nov\n",
    "                        convert.convert_and_save_brainflow_file_with_gap_filling(log, full_input_filename, full_output_filename, channels)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = \"Error processing file: \" + full_input_filename\n",
    "            log(msg)\n",
    "            log(e)\n",
    "            errors.append(msg)\n"
   ],
   "id": "21a84723f63a57a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "errors",
   "id": "b26100fa9d445409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed",
   "id": "5b5561ec7207ccd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run pipeline on FIF files",
   "id": "fd7176771a519eb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import run_feature_pipeline\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = ['2024-07-23-22-40-25', '2024-07-28-22-29-49', '2024-09-18-21-25-08', '2024-09-18-21-28-11', '2024-09-19-21-29-42']\n",
    "\n",
    "def process_file(root, dir_name):\n",
    "    input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "    if dir_name in skip_list:\n",
    "        log(f\"Skipping {dir_name}: \" + input_file)\n",
    "        return None\n",
    "    try:\n",
    "        log(f\"Processing file: \" + input_file)\n",
    "        if os.path.exists(input_file):\n",
    "            force = False\n",
    "            # Temp\n",
    "            # if dir_name.startswith(\"2024-11-2\"):\n",
    "            #     force = True\n",
    "            yasa_df = run_feature_pipeline.cached_pipeline(log, input_file, stats_df, force)\n",
    "            #log(f\"Returning {yasa_df.head()}\")\n",
    "            return yasa_df\n",
    "    except Exception as e:\n",
    "        msg = f\"Error processing file: \" + input_file + \" - \" + str(e)\n",
    "        log(msg)\n",
    "        errors.append(msg)\n",
    "        log(e)\n",
    "    return None\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    futures = []\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for dir_name in dirs:\n",
    "            futures.append(executor.submit(process_file, root, dir_name))\n",
    "\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        log(f\"Got result {result}\")\n",
    "        if result is not None:\n",
    "            dataframes.append(result)\n",
    "\n",
    "all = pd.concat(dataframes)\n",
    "log(f\"Finished processing, have {len(all)} files in total\")"
   ],
   "id": "92b7b91355d9b1f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "errors",
   "id": "9b4587583e3b58ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recalculate scalings\n",
    "N.b. can be run frequently but will only be picked up by new runs.  Maybe worth occasionally regenerating all old files.\n",
    "And yes, for new features have to rerun the pipeline on everything, then generate the stats here, then rerun the pipeline again on everything to have them use those."
   ],
   "id": "e8807e90215b2ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scaling import only_eeg\n",
    "\n",
    "only_eeg_cols = list(only_eeg(all).columns)\n",
    "assert any(col.startswith(\"Main\") for col in only_eeg_cols), \"No column starting with 'Main' found in only_eeg_cols\""
   ],
   "id": "124e12f519886246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scaling\n",
    "\n",
    "stats = scaling.stats(all)\n",
    "stats.to_csv(input_dir + \"/stats.csv\")\n",
    "assert any(stats['Column'].str.startswith(\"Main\")), \"No row starting with 'Main' found in column_name\""
   ],
   "id": "fdf438800f68bdd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Post-human processing\n",
    "This requires user interaction first to provide sleep times etc."
   ],
   "id": "59743d1b18996f06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sleep_events import load_days_data\n",
    "from sleep_events import pimp_my_days_data\n",
    "\n",
    "days_data = load_days_data(True)"
   ],
   "id": "a786118aa556417d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.eeg_states.eeg_states import load_and_prepare_settling_eeg_state_events\n",
    "\n",
    "tired_wired_eeg_state_events = load_and_prepare_settling_eeg_state_events()"
   ],
   "id": "8810c9b6d565e572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a453c74dcb57ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_post_human_pipeline import cached_post_human_pipeline\n",
    "import run_feature_pipeline\n",
    "\n",
    "dir_name = \"2024-12-12-21-20-17\"\n",
    "input_file = f\"C:\\\\dev\\\\play\\\\brainwave-data\\\\{dir_name}\\\\raw.fif\"\n",
    "row = days_data[days_data['dayAndNightOf'] == '2024-12-12']\n",
    "yasa_df = run_feature_pipeline.cached_pipeline(log, input_file, stats_df)\n",
    "post_human_df = cached_post_human_pipeline(log, dir_name, input_file, stats_df, days_data, yasa_df, tired_wired_eeg_state_events)\n",
    "post_human_df"
   ],
   "id": "dc932146086b8cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_post_human_pipeline import cached_post_human_pipeline\n",
    "\n",
    "errors = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(dirs):\n",
    "        input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        try:\n",
    "            log(\"Processing file: \" + input_file)\n",
    "            input_file_without_ext = os.path.splitext(input_file)[0]\n",
    "\n",
    "            if dir_name in skip_list:\n",
    "                log(f\"Skipping {idx} of {len(dirs)}: \" + input_file)\n",
    "                continue\n",
    "\n",
    "            if os.path.exists(input_file):\n",
    "                yasa_df = run_feature_pipeline.cached_pipeline(log, input_file, stats_df)\n",
    "                cached_post_human_pipeline(log, dir_name, input_file, stats_df, days_data, yasa_df, tired_wired_eeg_state_events)\n",
    "\n",
    "        except Exception as e:\n",
    "            log(\"Error processing file: \" + input_file)\n",
    "            errors.append(\"Error processing file: \" + input_file + \" - \" + str(e))\n",
    "            log(e)\n",
    "\n",
    "for err in errors:\n",
    "    log(err)\n"
   ],
   "id": "99d8e598cc4801c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6ba82e58d883561"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyse data completeness",
   "id": "fb165f2ba385f1aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "dfs = []\n",
    "modification_times = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(dirs):\n",
    "        input_file = os.path.join(root, dir_name, \"raw.post_human.csv\")\n",
    "        if os.path.exists(input_file):\n",
    "            df = pd.read_csv(input_file)\n",
    "            dfs.append(df)\n",
    "\n",
    "            # Get the last modification time of the file\n",
    "            mod_time = os.path.getmtime(input_file)\n",
    "            mod_time_dt = datetime.fromtimestamp(mod_time)\n",
    "            modification_times.append(mod_time_dt)\n",
    "\n",
    "# Concatenate all dataframes\n",
    "yasa_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Calculate the number of days from today for each modification time\n",
    "today = datetime.now()\n",
    "days_since_modification = [(today - mod_time).days for mod_time in modification_times]\n",
    "\n",
    "# Create a DataFrame with the modification times and days since modification\n",
    "modification_df = pd.DataFrame({\n",
    "    'file': [os.path.join(root, dir_name, \"raw.post_human.csv\") for root, dirs, files in os.walk(input_dir) for dir_name in dirs if os.path.exists(os.path.join(root, dir_name, \"raw.post_human.csv\"))],\n",
    "    'modification_time': modification_times,\n",
    "    'days_since_modification': days_since_modification\n",
    "})\n",
    "\n",
    "modification_df"
   ],
   "id": "4912068076cbcdcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upload to GCS",
   "id": "482f89663710fa04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from upload import upload_dir_to_gcs_skipping_existing\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for dir_name in reversed(dirs):\n",
    "        input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        full_dir_name = os.path.join(root, dir_name)\n",
    "        try:\n",
    "            upload_dir_to_gcs_skipping_existing(log, 'examined-life-derived-eeg', full_dir_name, dir_name)\n",
    "        except Exception as e:\n",
    "            log(\"Error processing file: \" + input_dir)\n",
    "            errors.append(\"Error processing file: \" + input_file + \" - \" + str(e))\n",
    "            log(e)\n",
    "\n",
    "for error in errors:\n",
    "    log(error)\n",
    "\n",
    "log(\"All uploaded\")"
   ],
   "id": "30732b799791e42d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if can delete Brainwave files that are safely backed up",
   "id": "6896e4e6423b928b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "can_delete = []\n",
    "cannot_delete = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if full_input_filename.endswith(\".brainflow.csv\"):\n",
    "            compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "                \n",
    "            if os.path.exists(compressed_full_output_filename):\n",
    "                can_delete.append({\n",
    "                    'backed_up': compressed_full_output_filename,\n",
    "                    'full_filename': full_input_filename\n",
    "                })\n",
    "            else:\n",
    "                cannot_delete.append(full_input_filename)\n",
    "    "
   ],
   "id": "54b1ee685c111770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "can_delete_df = pd.DataFrame(can_delete)\n",
    "can_delete_df"
   ],
   "id": "1b99c37e9a4b39e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cannot_delete",
   "id": "d7c0794b5a6e8351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "can_delete_df['full_filename']",
   "id": "83646d0ee8f59e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if can delete Cyton files that are safely backed up",
   "id": "555248de86908ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import webserver\n",
    "import bz2\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "errors = []\n",
    "can_delete = []\n",
    "cannot_delete = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if file_name.startswith(\"OBCI_\") and file_name.endswith(\".TXT.bz2\"):\n",
    "            can_delete.append({\n",
    "                'file_name': file_name.removesuffix(\".bz2\"),\n",
    "                'full_filename': full_input_filename\n",
    "            })"
   ],
   "id": "b436cf8c7ef3a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "can_delete",
   "id": "f6c68ce46d843ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cyton_file_locations = [input_dir, \"d:\", \"e:\", \"x:\"]\n",
    "\n",
    "for cyton_file_location in cyton_file_locations:\n",
    "    for root, dirs, files in os.walk(cyton_file_location):\n",
    "        for idx, file in enumerate(files):\n",
    "            matching_record = next((f for f in can_delete if f['file_name'] == file), None)\n",
    "            if matching_record:\n",
    "                full_filename = os.path.join(root, file)\n",
    "                log(f\"Could delete {full_filename} as backed up in {matching_record['full_filename']}\")"
   ],
   "id": "c73f7b9864e19c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import memory\n",
    "\n",
    "memory.garbage_collect(log)"
   ],
   "id": "80fb15db155e0758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ctypes\n",
    "\n",
    "ctypes.windll.user32.MessageBoxW(0, \"Compression is complete!\", \"Alert\", 0x40 | 0x1)"
   ],
   "id": "18263c6577d32646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4815003e12deaa7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d41cc48fcebc5da1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
