{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Autoreload possibly interferes with IntelliJ debugging\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "import logging\n",
    "# class FlushHandler(logging.StreamHandler):\n",
    "#     def emit(self, record):e\n",
    "#         super().emit(record)\n",
    "#         self.flush()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# log = lambda msg: logging.info(msg)\n",
    "log = lambda msg: print(f\"{datetime.today()} {msg}\")\n"
   ],
   "id": "34b2ec6a149ab9b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "\n",
   "id": "f89ac5fec644dc7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#assert False\n",
    "global_force = False"
   ],
   "id": "80044d81998b75e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full night pipeline (multiple files)",
   "id": "2177b708d94480ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_dir = \"C:\\\\dev\\\\play\\\\brainwave-data\"\n",
    "stats_df = pd.read_csv(input_dir + os.path.sep + \"stats.csv\")"
   ],
   "id": "4b29301dac33b36d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Find Brainflow files that haven't been copied over",
   "id": "a68b61c44f8dcdc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pytz\n",
    "from tqdm import tqdm\n",
    "import paramiko\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Define the time window\n",
    "time_window = timedelta(minutes=10)\n",
    "\n",
    "# Define the remote server details\n",
    "hostname = os.getenv('SSH_HOST')\n",
    "username = os.getenv('SSH_USERNAME')\n",
    "password = os.getenv('SSH_PASSWORD')\n",
    "remote_dir = '/home/graham/dev/Brainwave-Data'\n",
    "local_dir = input_dir\n",
    "\n",
    "# Create an SSH client\n",
    "ssh = paramiko.SSHClient()\n",
    "ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "ssh.connect(hostname, username=username, password=password, compress=True)\n",
    "\n",
    "# List files in the remote directory\n",
    "stdin, stdout, stderr = ssh.exec_command(f'ls {remote_dir}')\n",
    "remote_brainflow_files = [f for f in stdout.read().decode().splitlines() if f.endswith(\".brainflow.csv\")]\n"
   ],
   "id": "de7c66d190ca587d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "remote_brainflow_files",
   "id": "575825f9db239c2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "files_to_copy = remote_brainflow_files.copy()\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    #for idx, file_name in enumerate(tqdm(dirs, desc=\"Converting Brainflow to FIF\")):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if \".brainflow.csv.bz2\" in full_input_filename:\n",
    "            file_name_without_bz2 = file_name.removesuffix(\".bz2\")\n",
    "            already_have = file_name_without_bz2 in remote_brainflow_files\n",
    "            print(f\"Already have {full_input_filename}: {already_have}\")\n",
    "            if already_have:\n",
    "                files_to_copy.remove(file_name_without_bz2)\n",
    "\n"
   ],
   "id": "5cb4213ad55fa5c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "files_to_copy",
   "id": "423cd6c357456f29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import bz2\n",
    "from tqdm import tqdm\n",
    "\n",
    "sftp = ssh.open_sftp()\n",
    "for file in files_to_copy:\n",
    "    remote_file_path = remote_dir + \"/\" + file\n",
    "    local_file_path = os.path.join(input_dir, file)\n",
    "    log(f\"Copying Brainflow backup remote:{remote_file_path} to {local_file_path}\")\n",
    "\n",
    "    # Get the file size\n",
    "    remote_file_size = sftp.stat(remote_file_path).st_size\n",
    "\n",
    "    with tqdm(total=remote_file_size, unit='B', unit_scale=True, desc=file, ascii=True) as pbar:\n",
    "        local_file_path = os.path.join(input_dir, file)\n",
    "    \n",
    "        def callback(transferred_so_far, total_to_transfer):\n",
    "            pbar.update(transferred_so_far - pbar.n)\n",
    "    \n",
    "        # Create a file-like object that writes to the local file\n",
    "        with open(local_file_path, 'wb') as local_file:\n",
    "            sftp.getfo(remote_file_path, local_file, callback=callback)\n",
    "sftp.close()"
   ],
   "id": "cb454f01364aee73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "ssh.close()\n",
   "id": "42674aa9df2a9b98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert Brainflow files to FIF",
   "id": "6558e15223cc8efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_brainflow_compressed_filename(full_input_filename: str) -> str:\n",
    "    full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "    compressed_full_output_filename = str(os.path.join(full_output_dirname, os.path.basename(full_input_filename))) + '.bz2'\n",
    "    return compressed_full_output_filename"
   ],
   "id": "e93d35cf861b09fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import webserver\n",
    "import convert\n",
    "# import zstandard as zstd\n",
    "import os\n",
    "import bz2\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "errors = []\n",
    "processed = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = ['2024-09-10-21-22-21']\n",
    "\n",
    "def compress_bz2(input_file, output_file):\n",
    "    start_time = time.time()\n",
    "    with open(input_file, 'rb') as f_in:\n",
    "        with bz2.open(output_file, 'wb', compresslevel=9) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, os.path.getsize(output_file)\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    #for idx, file_name in enumerate(tqdm(dirs, desc=\"Converting Brainflow to FIF\")):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        try:\n",
    "            if full_input_filename.endswith(\".brainflow.csv\"):\n",
    "                full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "                full_output_filename = str(os.path.join(full_output_dirname, 'raw.fif'))\n",
    "                \n",
    "                compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "                \n",
    "                if not os.path.exists(compressed_full_output_filename):\n",
    "                    log(f\"Compressing file {full_input_filename} to \" + compressed_full_output_filename)\n",
    "                    processed.append(\"Compressing \" + full_input_filename)\n",
    "                    try:\n",
    "                        os.mkdir(os.path.dirname(compressed_full_output_filename))\n",
    "                    except:\n",
    "                        pass\n",
    "                    compress_bz2(full_input_filename, compressed_full_output_filename) \n",
    "                    \n",
    "                if os.path.exists(full_output_filename):\n",
    "                    log(f\"Skipping file {full_input_filename} as {full_output_filename} and {compressed_full_output_filename} already exist\")\n",
    "                    continue\n",
    "                should_skip = False\n",
    "                for s in skip_list:\n",
    "                    if s in full_input_filename:\n",
    "                        log(f\"Skipping file {full_input_filename}\")\n",
    "                        should_skip = True\n",
    "                if not should_skip:\n",
    "                    log(f\"Processing file {full_input_filename}\")\n",
    "                    processed.append(\"Processing \" + full_input_filename)\n",
    "                    channels = None\n",
    "                    date_time_str = os.path.basename(full_input_filename).removesuffix(\".brainflow.csv\")\n",
    "                    date_time_obj = datetime.strptime(date_time_str, '%Y-%m-%d-%H-%M-%S')\n",
    "                    if (date_time_obj > datetime(2024, 9, 1, 0, 0, 0)):\n",
    "                        channels = ['Fpz-M1']\n",
    "        \n",
    "                    if channels is not None:\n",
    "                        log(f\"Processing file {full_input_filename} with channels {channels}\")\n",
    "                        # Changing to gap-filling on 30th Nov\n",
    "                        convert.convert_and_save_brainflow_file_with_gap_filling(log, full_input_filename, full_output_filename, channels)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = \"Error processing file: \" + full_input_filename\n",
    "            log(msg)\n",
    "            log(e)\n",
    "            errors.append(msg)\n"
   ],
   "id": "21a84723f63a57a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "errors",
   "id": "b26100fa9d445409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed",
   "id": "5b5561ec7207ccd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run YASA pipeline on FIF files",
   "id": "fd7176771a519eb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import run_feature_pipeline\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = ['2024-07-23-22-40-25', '2024-07-28-22-29-49', '2024-09-18-21-25-08', '2024-09-18-21-28-11', '2024-09-19-21-29-42']\n",
    "import io\n",
    "import sys\n",
    "import contextlib\n",
    "import traceback\n",
    "from importlib import reload\n",
    "\n",
    "reload(run_feature_pipeline)\n",
    "\n",
    "def process_file(root, dir_name, force=False):\n",
    "    output_buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "\n",
    "        try:\n",
    "            input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "            if dir_name in skip_list:\n",
    "                log(f\"Skipping {dir_name}: {input_file}\")\n",
    "                return None, False, output_buffer.getvalue(), \"Skipped\"\n",
    "\n",
    "            log(f\"Processing file: {input_file}\")\n",
    "            if os.path.exists(input_file):\n",
    "                yasa_df, cached = run_feature_pipeline.cached_pipeline(log, input_file, force or global_force)\n",
    "                return yasa_df, cached, output_buffer.getvalue(), True\n",
    "            else:\n",
    "                log(f\"File not found: {input_file}\")\n",
    "                return None, False, output_buffer.getvalue(), \"File not found\"\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = f\"Error processing file: {input_file} - {e}\"\n",
    "            log(msg)\n",
    "            errors.append(msg)\n",
    "            return None, False, output_buffer.getvalue(), \"Error: \" + str(e)\n",
    "\n",
    "# Testing\n",
    "# yasa_df, cached, stdout, success_or_msg = process_file(\"C:\\\\dev\\\\play\\\\brainwave-data\", \"2024-07-16-23-14-52\", False)\n",
    "# stdout, success_or_msg"
   ],
   "id": "92b7b91355d9b1f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame(columns=['dir_name', 'cached', 'stdout', 'success_or_msg'])\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(tqdm(dirs, desc=\"YASA pipeline\")):\n",
    "        yasa_df, cached, stdout, success_or_msg = process_file(root, dir_name)\n",
    "        results.loc[len(results)] = [dir_name, cached, stdout, success_or_msg]\n",
    "\n",
    "results"
   ],
   "id": "3742c3bb21229387",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Post-YASA pipeline",
   "id": "e25d23e0136d1aa1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:32:01.802128Z",
     "start_time": "2024-12-14T11:32:01.785663Z"
    }
   },
   "cell_type": "markdown",
   "source": "",
   "id": "6878d2eafad5100"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import traceback\n",
    "import os\n",
    "\n",
    "from importlib import reload\n",
    "import run_post_yasa_pipeline\n",
    "reload(run_post_yasa_pipeline)\n",
    "\n",
    "\n",
    "def process_file_for_post_yasa(root, dir_name, force=False):\n",
    "    output_buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "\n",
    "        input_fif_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        input_csv_file = os.path.join(root, dir_name, \"raw.yasa.csv\")\n",
    "        try:\n",
    "            log(f\"Processing file: \" + input_fif_file)\n",
    "            if os.path.exists(input_fif_file) and os.path.exists(input_csv_file):\n",
    "                yasa_df = pd.read_csv(input_csv_file)\n",
    "                post_yasa_df, cached = run_post_yasa_pipeline.cached_post_yasa_pipeline(log, input_fif_file, yasa_df, stats_df, force or global_force)\n",
    "                return post_yasa_df, cached, output_buffer.getvalue(), True\n",
    "            else:\n",
    "                log(f\"File not found: {input_fif_file} or {input_csv_file}\")\n",
    "                return None, False, output_buffer.getvalue(), \"File not found\"\n",
    "        except Exception as e:\n",
    "            return None, False, output_buffer.getvalue(), e\n",
    "\n",
    "# Testing\n",
    "# post_yasa_df, cached, stdout, success_or_msg = process_file_for_post_yasa(\"C:\\\\dev\\\\play\\\\brainwave-data\", \"2024-07-16-23-14-52\", False)\n",
    "# display(stdout)\n",
    "# display(success_or_msg)"
   ],
   "id": "ac4c838f3988591c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import yasa_features\n",
    "# import mne\n",
    "# import scaling\n",
    "# import convert\n",
    "#     # Load MNE\n",
    "#\n",
    "# yasa_df = pd.read_csv(\"C:\\\\dev\\\\play\\\\brainwave-data\\\\2024-07-16-23-14-52\\\\raw.yasa.csv\")\n",
    "# mne.use_log_level(\"warning\")\n",
    "# raw, input_file_without_ext, mne_filtered = convert.load_mne_file(log, \"C:\\\\dev\\\\play\\\\brainwave-data\\\\2024-07-16-23-14-52\\\\raw.fif\")\n",
    "#\n",
    "# channels = raw.info['ch_names']\n",
    "# sfreq = raw.info['sfreq']\n",
    "# start_date = raw.info['meas_date']\n",
    "# end_date = start_date + timedelta(seconds=float(raw.times[-1]))\n",
    "#\n",
    "# # Sleep events - v expensive, needs pushdown filter, not used for much\n",
    "# # garbage_collect(log)\n",
    "# # log(\"Loading sleep events\")\n",
    "# # ha_events = sleep_events.load_sleep_events(log, start_date, end_date)\n",
    "# # output_csv_file = input_file_without_ext + \".night_events.csv\"\n",
    "# # ha_events.to_csv(output_csv_file, index=False)\n",
    "#\n",
    "#\n",
    "# # YASA features\n",
    "# log(\"Extracting YASA features\")\n",
    "# yasa_feats, channel_feats_dict = yasa_features.extract_yasa_features2(log, channels, mne_filtered)\n",
    "#\n",
    "# print(\"YASA_df: \", yasa_df.columns)\n",
    "# print(\"yasa_feats: \", yasa_feats.columns)\n",
    "#\n",
    "# # # Combine epochs and YASA features\n",
    "# # garbage_collect(log)\n",
    "# # df = yasa_feats.copy()\n",
    "# # df['epoch'] = df['Epoch']\n",
    "# # df.set_index('epoch', inplace=True)\n",
    "# combined_df = yasa_df.join(yasa_feats)\n",
    "#\n",
    "# print(\"combined_df: \", combined_df.columns)\n",
    "#\n",
    "# # Scaled\n",
    "# scale_by_stats = scaling.scale_by_stats(combined_df, stats_df)\n"
   ],
   "id": "dc02fbc7ade361a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame(columns=['dir_name', 'cached', 'stdout', 'success_or_msg'])\n",
    "\n",
    "all_post_yasa_dfs = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(tqdm(dirs, desc=\"Post YASA pipeline\")):\n",
    "        post_yasa_df, cached, stdout, success_or_msg = process_file_for_post_yasa(root, dir_name)\n",
    "        all_post_yasa_dfs.append(post_yasa_df)\n",
    "        results.loc[len(results)] = [dir_name, cached, stdout, success_or_msg]\n",
    "        # error_count = len(results[results['success_or_msg'] != True])\n",
    "        # if (error_count > 5):\n",
    "        #     log(f\"Stopping due to {error_count} errors\")\n",
    "        #     break\n",
    "\n",
    "results\n"
   ],
   "id": "83319905fde09809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recalculate scalings\n",
    "N.b. can be run frequently but will only be picked up by new runs.  Maybe worth occasionally regenerating all old files.\n",
    "And yes, for new features have to rerun the pipeline on everything, then generate the stats here, then rerun the pipeline again on everything to have them use those."
   ],
   "id": "4c487ce7a385265e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "post_yasa_dfs_combined = pd.concat(all_post_yasa_dfs)",
   "id": "60c194d4c115d030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from scaling import only_eeg\n",
    "#\n",
    "# only_eeg_cols = list(only_eeg(post_yasa_dfs_combined).columns)\n",
    "# assert any(col.startswith(\"Main\") for col in only_eeg_cols), \"No column starting with 'Main' found in only_eeg_cols\""
   ],
   "id": "ea7c30608a88ddd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Expensive to do every time now\n",
    "# import scaling\n",
    "# reload(scaling)\n",
    "#\n",
    "# stats = scaling.stats(post_yasa_dfs_combined)\n",
    "# stats.to_csv(input_dir + \"/stats.csv\")\n",
    "# # assert any(stats['Column'].str.startswith(\"Main\")), \"No row starting with 'Main' found in column_name\"\n",
    "# stats"
   ],
   "id": "b45325b82b166f42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-14T14:21:26.533907Z"
    }
   },
   "cell_type": "markdown",
   "source": "# Microwakings",
   "id": "6e825bb10b9e135a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.microwakings_1 import microwakings_pipeline\n",
    "import os\n",
    "\n",
    "from importlib import reload\n",
    "reload(microwakings_pipeline)\n",
    "\n",
    "def process_microwakings(root, dir_name, force=False):\n",
    "    output_buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "        input_fif_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        input_csv_file = os.path.join(root, dir_name, \"raw.post_yasa.csv\")\n",
    "        try:\n",
    "            log(f\"Processing file: \" + input_fif_file)\n",
    "            if not os.path.exists(input_fif_file):\n",
    "                return None, False, output_buffer.getvalue(), \"File not found \" + input_fif_file\n",
    "            if not os.path.exists(input_csv_file):\n",
    "                return None, False, output_buffer.getvalue(), \"File not found \" + input_csv_file\n",
    "\n",
    "            post_yasa_df = pd.read_csv(input_csv_file)\n",
    "            microwakings_df, cached = microwakings_pipeline.cached_microwakings_pipeline(log, input_fif_file, post_yasa_df, force or global_force)\n",
    "            return microwakings_df, cached, output_buffer.getvalue(), True\n",
    "        except Exception as e:\n",
    "            return None, False, output_buffer.getvalue(), \"Error: \" + str(e)\n",
    "\n",
    "# Testing\n",
    "# microwakings_df, cached, stdout, success_or_msg = process_microwakings(\"C:\\\\dev\\\\play\\\\brainwave-data\", \"2024-11-29-21-18-33\", False)\n",
    "# stdout, success_or_msg"
   ],
   "id": "d62e36ec5e5980cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame(columns=['dir_name', 'cached', 'stdout', 'success_or_msg'])\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(tqdm(dirs, desc=\"Microwakings pipeline\")):\n",
    "        microwakings_df, cached, stdout, success_or_msg = process_microwakings(root, dir_name)\n",
    "        results.loc[len(results)] = [dir_name, cached, stdout, success_or_msg]\n",
    "\n",
    "results"
   ],
   "id": "32e5f01fcd1ed033",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Post-human pipeline\n",
    "This requires user interaction first to provide sleep times etc."
   ],
   "id": "7ebf4c256a66aab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sleep_events import load_days_data\n",
    "from sleep_events import pimp_my_days_data\n",
    "\n",
    "days_data = load_days_data(True)"
   ],
   "id": "224421aa114f231b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from models.eeg_states.eeg_states import load_and_prepare_settling_eeg_state_events\n",
    "\n",
    "tired_wired_eeg_state_events = load_and_prepare_settling_eeg_state_events()"
   ],
   "id": "a786118aa556417d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import run_post_human_pipeline\n",
    "reload(run_post_human_pipeline)\n",
    "from run_post_human_pipeline import cached_post_human_pipeline\n",
    "\n",
    "def process_file_for_post_human(root, dir_name, force=False):\n",
    "    output_buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "\n",
    "        input_fif_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        input_csv_file = os.path.join(root, dir_name, \"raw.yasa.csv\")\n",
    "        try:\n",
    "            log(f\"Processing file: \" + input_fif_file)\n",
    "            if not os.path.exists(input_fif_file):\n",
    "                return None, False, output_buffer.getvalue(), \"File not found \" + input_fif_file\n",
    "            if not os.path.exists(input_csv_file):\n",
    "                return None, False, output_buffer.getvalue(), \"File not found \" + input_csv_file\n",
    "\n",
    "            post_yasa_df = pd.read_csv(input_csv_file)\n",
    "            post_human_df, cached = cached_post_human_pipeline(log, dir_name, input_fif_file, stats_df, days_data, post_yasa_df, tired_wired_eeg_state_events, force or global_force)\n",
    "            return post_human_df, cached, output_buffer.getvalue(), True\n",
    "        except Exception as e:\n",
    "            return None, False, output_buffer.getvalue(), \"Error: \" + str(e)\n",
    "\n",
    "# Testing\n",
    "# post_human_df, cached, stdout, success_or_msg = process_file_for_post_human(\"C:\\\\dev\\\\play\\\\brainwave-data\", \"2024-11-29-21-18-33\", False)\n",
    "# stdout, success_or_msg"
   ],
   "id": "8810c9b6d565e572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a453c74dcb57ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# For testing\n",
    "# from run_post_human_pipeline import cached_post_human_pipeline\n",
    "# import run_feature_pipeline\n",
    "#\n",
    "# dir_name = \"2024-12-12-21-20-17\"\n",
    "# input_file = f\"C:\\\\dev\\\\play\\\\brainwave-data\\\\{dir_name}\\\\raw.fif\"\n",
    "# row = days_data[days_data['dayAndNightOf'] == '2024-12-12']\n",
    "# yasa_df = run_feature_pipeline.cached_pipeline(log, input_file, stats_df)\n",
    "# post_human_df = cached_post_human_pipeline(log, dir_name, input_file, stats_df, days_data, yasa_df, tired_wired_eeg_state_events)\n",
    "# post_human_df"
   ],
   "id": "5f47d02e2265a17c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from run_post_human_pipeline import cached_post_human_pipeline\n",
    "\n",
    "results = pd.DataFrame(columns=['dir_name', 'cached', 'stdout', 'success_or_msg'])\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(tqdm(dirs, desc=\"Post human pipeline\")):\n",
    "        post_human_df, cached, stdout, success_or_msg = process_file_for_post_human(root, dir_name)\n",
    "        results.loc[len(results)] = [dir_name, cached, stdout, success_or_msg]\n",
    "\n",
    "results"
   ],
   "id": "dc932146086b8cb0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Stitch all pipelines together",
   "id": "a5ae5c62e5b1eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_pipeline_stitch(root, dir_name, force=False):\n",
    "    output_buffer = io.StringIO()\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "\n",
    "        post_yasa_file = os.path.join(root, dir_name, \"raw.post_yasa.csv\")\n",
    "        post_human_file = os.path.join(root, dir_name, \"raw.post_human.csv\")\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(post_yasa_file):\n",
    "                # Multiple consumers expect this file\n",
    "                with_features = os.path.join(root, dir_name, \"raw.with_features.csv\")\n",
    "                if not os.path.exists(with_features) or force:\n",
    "                    post_yasa_df = pd.read_csv(post_yasa_file)\n",
    "                    post_yasa_df.to_csv(with_features, index=False)\n",
    "\n",
    "            return output_buffer.getvalue(), True\n",
    "        except Exception as e:\n",
    "            return output_buffer.getvalue(), \"Error: \" + str(e)\n",
    "\n",
    "\n",
    "# Testing\n",
    "stdout, success_or_msg = process_pipeline_stitch(\"C:\\\\dev\\\\play\\\\brainwave-data\", \"2024-12-14-21-26-54\", False)\n",
    "stdout, success_or_msg\n"
   ],
   "id": "820967d55d46177c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = pd.DataFrame(columns=['dir_name', 'stdout', 'success_or_msg'])\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, dir_name in enumerate(tqdm(dirs, desc=\"Stitching pipelines\")):\n",
    "        stdout, success_or_msg = process_pipeline_stitch(root, dir_name)\n",
    "        results.loc[len(results)] = [dir_name, stdout, success_or_msg]\n",
    "\n",
    "results"
   ],
   "id": "574c63e466850c33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Analyse data completeness",
   "id": "fb165f2ba385f1aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "#\n",
    "# dfs = []\n",
    "# modification_times = []\n",
    "#\n",
    "# for root, dirs, files in os.walk(input_dir):\n",
    "#     for idx, dir_name in enumerate(dirs):\n",
    "#         input_file = os.path.join(root, dir_name, \"raw.post_human.csv\")\n",
    "#         if os.path.exists(input_file):\n",
    "#             df = pd.read_csv(input_file)\n",
    "#             dfs.append(df)\n",
    "#\n",
    "#             # Get the last modification time of the file\n",
    "#             mod_time = os.path.getmtime(input_file)\n",
    "#             mod_time_dt = datetime.fromtimestamp(mod_time)\n",
    "#             modification_times.append(mod_time_dt)\n",
    "#\n",
    "# # Concatenate all dataframes\n",
    "# yasa_df = pd.concat(dfs, ignore_index=True)\n",
    "#\n",
    "# # Calculate the number of days from today for each modification time\n",
    "# today = datetime.now()\n",
    "# days_since_modification = [(today - mod_time).days for mod_time in modification_times]\n",
    "#\n",
    "# # Create a DataFrame with the modification times and days since modification\n",
    "# modification_df = pd.DataFrame({\n",
    "#     'file': [os.path.join(root, dir_name, \"raw.post_human.csv\") for root, dirs, files in os.walk(input_dir) for dir_name in dirs if os.path.exists(os.path.join(root, dir_name, \"raw.post_human.csv\"))],\n",
    "#     'modification_time': modification_times,\n",
    "#     'days_since_modification': days_since_modification\n",
    "# })\n",
    "#\n",
    "# modification_df"
   ],
   "id": "4912068076cbcdcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upload to GCS",
   "id": "482f89663710fa04"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from upload import upload_dir_to_gcs_skipping_existing\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for dir_name in reversed(dirs):\n",
    "        input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        full_dir_name = os.path.join(root, dir_name)\n",
    "        try:\n",
    "            upload_dir_to_gcs_skipping_existing(log, 'examined-life-derived-eeg', full_dir_name, dir_name)\n",
    "        except Exception as e:\n",
    "            log(\"Error processing file: \" + input_dir)\n",
    "            errors.append(\"Error processing file: \" + input_file + \" - \" + str(e))\n",
    "            log(e)\n",
    "\n",
    "for error in errors:\n",
    "    log(error)\n",
    "\n",
    "log(\"All uploaded\")"
   ],
   "id": "30732b799791e42d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if can delete Brainwave files that are safely backed up",
   "id": "6896e4e6423b928b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "can_delete = []\n",
    "cannot_delete = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if full_input_filename.endswith(\".brainflow.csv\"):\n",
    "            compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "                \n",
    "            if os.path.exists(compressed_full_output_filename):\n",
    "                can_delete.append({\n",
    "                    'backed_up': compressed_full_output_filename,\n",
    "                    'full_filename': full_input_filename\n",
    "                })\n",
    "            else:\n",
    "                cannot_delete.append(full_input_filename)\n",
    "    "
   ],
   "id": "54b1ee685c111770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "can_delete_df = pd.DataFrame(can_delete)\n",
    "can_delete_df"
   ],
   "id": "1b99c37e9a4b39e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cannot_delete",
   "id": "d7c0794b5a6e8351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "can_delete_df['full_filename']",
   "id": "83646d0ee8f59e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if can delete Cyton files that are safely backed up",
   "id": "555248de86908ae3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import webserver\n",
    "import bz2\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "errors = []\n",
    "can_delete = []\n",
    "cannot_delete = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if file_name.startswith(\"OBCI_\") and file_name.endswith(\".TXT.bz2\"):\n",
    "            can_delete.append({\n",
    "                'file_name': file_name.removesuffix(\".bz2\"),\n",
    "                'full_filename': full_input_filename\n",
    "            })"
   ],
   "id": "b436cf8c7ef3a74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "can_delete",
   "id": "f6c68ce46d843ec5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cyton_file_locations = [input_dir, \"d:\", \"e:\", \"x:\"]\n",
    "\n",
    "for cyton_file_location in cyton_file_locations:\n",
    "    for root, dirs, files in os.walk(cyton_file_location):\n",
    "        for idx, file in enumerate(files):\n",
    "            matching_record = next((f for f in can_delete if f['file_name'] == file), None)\n",
    "            if matching_record:\n",
    "                full_filename = os.path.join(root, file)\n",
    "                log(f\"Could delete {full_filename} as backed up in {matching_record['full_filename']}\")"
   ],
   "id": "c73f7b9864e19c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import memory\n",
    "\n",
    "memory.garbage_collect(log)"
   ],
   "id": "80fb15db155e0758",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import ctypes\n",
    "\n",
    "ctypes.windll.user32.MessageBoxW(0, \"Compression is complete!\", \"Alert\", 0x40 | 0x1)"
   ],
   "id": "18263c6577d32646",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4815003e12deaa7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d41cc48fcebc5da1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
