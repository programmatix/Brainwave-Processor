{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import tqdm\n",
    "# Autoreload possibly interferes with IntelliJ debugging\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "class FlushHandler(logging.StreamHandler):\n",
    "    def emit(self, record):\n",
    "        super().emit(record)\n",
    "        self.flush()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[FlushHandler()])\n",
    "log = lambda msg: logging.info(msg)\n"
   ],
   "id": "34b2ec6a149ab9b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Full pipeline (multiple files)",
   "id": "2177b708d94480ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "input_dir = \"C:\\\\dev\\\\play\\\\brainwave-data-day\"\n",
    "stats_df = pd.read_csv(input_dir + os.path.sep + \"day_stats.csv\")\n",
    "# stats_df = pd.read_csv(\"C:\\\\dev\\\\play\\\\brainwave-data\\\\stats.csv\")"
   ],
   "id": "4b29301dac33b36d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import models.eeg_states.eeg_states\n",
    "# reload(models.eeg_states.eeg_states)\n",
    "# from models.eeg_states.eeg_states import load_and_prepare_day_data_energy_eeg_state_events\n",
    "#\n",
    "# events = load_and_prepare_day_data_energy_eeg_state_events()"
   ],
   "id": "5d29f83733c0a0c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Convert Brainflow files to FIF",
   "id": "6558e15223cc8efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_brainflow_compressed_filename(full_input_filename: str) -> str:\n",
    "    full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "    compressed_full_output_filename = str(os.path.join(full_output_dirname, os.path.basename(full_input_filename))) + '.bz2'\n",
    "    return compressed_full_output_filename"
   ],
   "id": "e93d35cf861b09fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import webserver\n",
    "import convert\n",
    "# import zstandard as zstd\n",
    "import os\n",
    "import bz2\n",
    "import time\n",
    "import shutil\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "errors = []\n",
    "processed = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = []\n",
    "\n",
    "force = False\n",
    "\n",
    "def compress_bz2(input_file, output_file):\n",
    "    start_time = time.time()\n",
    "    with open(input_file, 'rb') as f_in:\n",
    "        with bz2.open(output_file, 'wb', compresslevel=9) as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, os.path.getsize(output_file)\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    # Exclude the last file, which we assume to be the most recent, and possibly still being written\n",
    "    files = [file for file in files if file.endswith(\".brainflow.csv\")][:-1]\n",
    "    for idx, file_name in tqdm(enumerate(files), desc=\"Processing directories\", total=(len(files))):  \n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        try:\n",
    "            full_output_dirname = webserver.output_dirname(full_input_filename)\n",
    "            full_output_filename = str(os.path.join(full_output_dirname, 'raw.fif'))\n",
    "            \n",
    "            compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "            \n",
    "            if not os.path.exists(compressed_full_output_filename) or force:\n",
    "                log(f\"Compressing file {full_input_filename} to \" + compressed_full_output_filename)\n",
    "                processed.append(\"Compressing \" + full_input_filename)\n",
    "                try:\n",
    "                    os.mkdir(os.path.dirname(compressed_full_output_filename))\n",
    "                except:\n",
    "                    pass\n",
    "                compress_bz2(full_input_filename, compressed_full_output_filename) \n",
    "                \n",
    "            if os.path.exists(full_output_filename) and not force:\n",
    "                log(f\"Skipping file {full_input_filename} as {full_output_filename} and {compressed_full_output_filename} already exist\")\n",
    "                continue\n",
    "            should_skip = False\n",
    "            for s in skip_list:\n",
    "                if s in full_input_filename:\n",
    "                    log(f\"Skipping file {full_input_filename}\")\n",
    "                    should_skip = True\n",
    "            if not should_skip:\n",
    "                log(f\"Processing file {full_input_filename}\")\n",
    "                processed.append(\"Processing \" + full_input_filename)\n",
    "                channels = ['Fpz-M1']\n",
    "                date_time_str = os.path.basename(full_input_filename).removesuffix(\".brainflow.csv\")\n",
    "                date_time_obj = datetime.strptime(date_time_str, '%Y-%m-%d-%H-%M-%S')\n",
    "    \n",
    "                if channels is not None:\n",
    "                    log(f\"Processing file {full_input_filename} with channels {channels}\")\n",
    "                    convert.convert_and_save_brainflow_file_with_gap_filling(log, full_input_filename, full_output_filename, channels)\n",
    "\n",
    "        except Exception as e:\n",
    "            msg = \"Error processing file: \" + full_input_filename\n",
    "            log(msg)\n",
    "            log(e)\n",
    "            errors.append(msg)\n"
   ],
   "id": "21a84723f63a57a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "errors"
   ],
   "id": "b26100fa9d445409",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed",
   "id": "5b5561ec7207ccd0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Run pipeline on FIF files",
   "id": "fd7176771a519eb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import contextlib\n",
    "import io\n",
    "import run_day_pipeline\n",
    "from importlib import reload\n",
    "reload(run_day_pipeline)\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "# Could get these working later\n",
    "skip_list = []\n",
    "\n",
    "def process_file(root, dir_name):\n",
    "    input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "    if dir_name in skip_list:\n",
    "        log(f\"Skipping {dir_name}: \" + input_file)\n",
    "        return None\n",
    "    try:\n",
    "        log(f\"Processing file: \" + input_file)\n",
    "        if os.path.exists(input_file):\n",
    "            yasa_df = run_day_pipeline.cached_pipeline(log, input_file, stats_df, events)\n",
    "            #log(f\"Returning {yasa_df.head()}\")\n",
    "            return yasa_df\n",
    "    except Exception as e:\n",
    "        msg = f\"Error processing file: \" + input_file + \" - \" + str(e)\n",
    "        log(msg)\n",
    "        errors.append(msg)\n",
    "        log(e)\n",
    "    return None\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for dir_name in tqdm(dirs, desc=\"Processing directories\", total=len(dirs)):\n",
    "        output_buffer = io.StringIO()\n",
    "        with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "            df = process_file(root, dir_name)\n",
    "            if df is not None:\n",
    "                dataframes.append(df)\n",
    "\n",
    "all = pd.concat(dataframes)\n",
    "log(f\"Finished processing, have {len(all)} files in total\")"
   ],
   "id": "92b7b91355d9b1f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataframes",
   "id": "15154612e4a627df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "errors",
   "id": "9b4587583e3b58ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recalculate scalings\n",
    "N.b. can be run frequently but will only be picked up by new runs.  Maybe worth occasionally regenerating all old files.\n",
    "And yes, for new features have to rerun the pipeline on everything, then generate the stats here, then rerun the pipeline again on everything to have them use those."
   ],
   "id": "e8807e90215b2ad8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scaling import only_eeg\n",
    "\n",
    "only_eeg_cols = list(only_eeg(all).columns)\n",
    "assert any(col.startswith(\"Main\") for col in only_eeg_cols), \"No column starting with 'Main' found in only_eeg_cols\""
   ],
   "id": "124e12f519886246",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import scaling\n",
    "\n",
    "stats = scaling.stats(all)\n",
    "stats.to_csv(input_dir + \"/day_stats.csv\")\n",
    "assert any(stats['Column'].str.startswith(\"Main\")), \"No row starting with 'Main' found in column_name\"\n",
    "stats"
   ],
   "id": "fdf438800f68bdd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Upload to GCS",
   "id": "482f89663710fa04"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "from upload import upload_dir_to_gcs_skipping_existing\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "dataframes = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for dir_name in tqdm(reversed(dirs), desc=\"Uploading directories\", total=len(dirs)):\n",
    "        full_dir_name = os.path.join(root, dir_name)\n",
    "        try:\n",
    "            upload_dir_to_gcs_skipping_existing(log, 'examined-life-input-eeg-day', full_dir_name, dir_name)\n",
    "        except Exception as e:\n",
    "            log(\"Error processing file: \" + input_dir)\n",
    "            log(e)\n",
    "\n",
    "for error in errors:\n",
    "    log(error)\n",
    "\n",
    "log(\"All uploaded\")"
   ],
   "id": "30732b799791e42d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check if can delete Brainwave files that are safely backed up",
   "id": "6896e4e6423b928b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "\n",
    "errors = []\n",
    "can_delete = []\n",
    "cannot_delete = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for idx, file_name in enumerate(files):\n",
    "        full_input_filename = os.path.join(root, file_name)\n",
    "        if full_input_filename.endswith(\".brainflow.csv\"):\n",
    "            compressed_full_output_filename = get_brainflow_compressed_filename(full_input_filename)\n",
    "                \n",
    "            if os.path.exists(compressed_full_output_filename):\n",
    "                can_delete.append({\n",
    "                    'backed_up': compressed_full_output_filename,\n",
    "                    'full_filename': full_input_filename\n",
    "                })\n",
    "            else:\n",
    "                cannot_delete.append(full_input_filename)\n",
    "    "
   ],
   "id": "54b1ee685c111770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "can_delete_df = pd.DataFrame(can_delete)\n",
    "can_delete_df"
   ],
   "id": "1b99c37e9a4b39e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cannot_delete",
   "id": "d7c0794b5a6e8351",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "can_delete_df['full_filename']",
   "id": "83646d0ee8f59e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6b3d57c7e163ee43",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
