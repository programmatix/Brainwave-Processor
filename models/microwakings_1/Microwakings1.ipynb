{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Microwakings Model Single File\n",
    "Training a model to detect microwakings, on a single file.\n",
    "Superseeded by MicrowakingsMulti1.ipynb"
   ],
   "id": "50381d01370cd08f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:57:48.597613Z",
     "start_time": "2024-09-12T11:57:48.538763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "log = lambda msg: logging.info(msg)\n"
   ],
   "id": "1a418d2b165cbd6c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "8329a7b6166610f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dac3d377d7045c85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:57:49.540463Z",
     "start_time": "2024-09-12T11:57:49.496966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_file = \"C:\\\\dev\\\\play\\\\brainwave-data\\\\2024-07-16-23-14-52\\\\raw.fif\"\n",
    "input_file_without_ext = os.path.splitext(input_file)[0]\n"
   ],
   "id": "78323911b7120f8f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:57:49.836346Z",
     "start_time": "2024-09-12T11:57:49.798773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import sys\n",
    "# import convert\n",
    "# \n",
    "# raw, input_file_without_ext, mne_filtered = convert.load_mne_file(log, input_file)"
   ],
   "id": "1ada5d951c32e37a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:57:57.026141Z",
     "start_time": "2024-09-12T11:57:50.714713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif(input_file, preload=True)\n",
    "filtered = raw.copy()\n",
    "\n",
    "# AASM recommendation\n",
    "filtered.filter(0.3, 35)\n",
    "\n",
    "filtered.notch_filter(freqs=[50,100])\n",
    "\n",
    "# Bit confused about this, something to do with MNE storing in volts.  But YASA complains it doesn't look uV if I don't do this.\n",
    "data = filtered.get_data(units=dict(eeg=\"uV\")) / 1_000_000\n",
    "filtered._data = data\n",
    "mne_filtered = filtered\n",
    "\n",
    "input_file_without_ext = os.path.splitext(input_file)[0]\n"
   ],
   "id": "4fd9dc40763d125f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file C:\\dev\\play\\brainwave-data\\2024-07-16-23-14-52\\raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 7403887 =      0.000 ... 29615.548 secs\n",
      "Ready.\n",
      "Reading 0 ... 7403887  =      0.000 ... 29615.548 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 0.3 - 35 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 0.30\n",
      "- Lower transition bandwidth: 0.30 Hz (-6 dB cutoff frequency: 0.15 Hz)\n",
      "- Upper passband edge: 35.00 Hz\n",
      "- Upper transition bandwidth: 8.75 Hz (-6 dB cutoff frequency: 39.38 Hz)\n",
      "- Filter length: 2751 samples (11.004 s)\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 1651 samples (6.604 s)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:57:58.137077Z",
     "start_time": "2024-09-12T11:57:57.028142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file_path = input_file.replace('.fif', '.scorings.json')\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "df = pd.json_normalize(data, 'marks', errors='ignore')\n",
    "df"
   ],
   "id": "3804b4d9d6d4878a",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\dev\\\\play\\\\brainwave-data\\\\2024-07-16-23-14-52\\\\raw.scorings.json'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      4\u001B[0m json_file_path \u001B[38;5;241m=\u001B[39m input_file\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.fif\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.scorings.json\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mjson_file_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m      6\u001B[0m     data \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(file)\n\u001B[0;32m      7\u001B[0m df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mjson_normalize(data, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmarks\u001B[39m\u001B[38;5;124m'\u001B[39m, errors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\dev\\\\play\\\\brainwave-data\\\\2024-07-16-23-14-52\\\\raw.scorings.json'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare wakings",
   "id": "bacf42f2c931823b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "microwakings = df.copy()\n",
    "# Convert timestamp and scoredAt to datetime\n",
    "microwakings['timestamp'] = pd.to_datetime(microwakings['timestamp'])\n",
    "microwakings['scoredAt'] = pd.to_datetime(microwakings['scoredAt'])\n",
    "\n",
    "# Convert type and channel to string\n",
    "microwakings['type'] = microwakings['type'].astype(str)\n",
    "microwakings['channel'] = microwakings['channel'].astype(str)\n",
    "microwakings"
   ],
   "id": "e41dd48dc5e9ce0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify the most frequent channel\n",
    "most_frequent_channel = microwakings['channel'].value_counts().idxmax()\n",
    "\n",
    "# Filter the DataFrame to only include rows with the most frequent channel\n",
    "microwakings = microwakings[microwakings['channel'] == most_frequent_channel]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "microwakings"
   ],
   "id": "a8bcda8c797ebe99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store matched microwakings\n",
    "matched_microwakings = []\n",
    "\n",
    "# Loop through the DataFrame to find matching MicrowakingStart and MicrowakingEnd\n",
    "for i, start_row in microwakings[microwakings['type'] == 'MicrowakingStart'].iterrows():\n",
    "    start_time = start_row['timestamp']\n",
    "    for j, end_row in microwakings[microwakings['type'] == 'MicrowakingEnd'].iterrows():\n",
    "        end_time = end_row['timestamp']\n",
    "        if start_time <= end_time <= start_time + pd.Timedelta(minutes=2):\n",
    "            matched_microwakings.append((start_time, end_time))\n",
    "            break  # Assuming one-to-one matching\n",
    "\n",
    "# Convert the matched microwakings to a DataFrame if needed\n",
    "microwakings_df = pd.DataFrame(matched_microwakings, columns=['Start', 'End'])\n",
    "microwakings_df['Duration'] = microwakings_df['End'] - microwakings_df['Start']\n",
    "\n",
    "# Display the matched DataFrame\n",
    "microwakings_df"
   ],
   "id": "433810ce268bf816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "microwakings_df['Duration'].describe()",
   "id": "2e291e2802828175",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare model data",
   "id": "94818507543250fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "resampled = mne_filtered.copy()\n",
    "# 100 hz is very similar to 250 hz to naked eye.\n",
    "resampled_rate = 10\n",
    "resampled.resample(resampled_rate, npad=\"auto\")\n",
    "eeg_data = resampled.get_data(picks = most_frequent_channel, units=dict(eeg=\"uV\"))\n",
    "eeg_data.shape\n"
   ],
   "id": "b7aa1582717be8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract start and end times from mne_filtered\n",
    "start_time = mne_filtered.info['meas_date']\n",
    "num_samples = eeg_data.shape[1]\n",
    "end_time = start_time + pd.Timedelta(seconds=num_samples / resampled_rate)\n",
    "\n",
    "# Step 2: Create a DataFrame with timestamps\n",
    "timestamps = pd.date_range(start=start_time, periods=num_samples, freq=pd.Timedelta(seconds=1/resampled_rate))\n",
    "labels_df = pd.DataFrame({'timestamp': timestamps, 'Microwaking': 0})\n",
    "\n",
    "# Step 3: Label the timestamps\n",
    "for _, row in microwakings_df.iterrows():\n",
    "    labels_df.loc[(labels_df['timestamp'] >= row['Start']) & (labels_df['timestamp'] <= row['End']), 'Microwaking'] = 1\n",
    "\n",
    "# Step 4: Convert the DataFrame to a NumPy array\n",
    "labels = labels_df['Microwaking'].to_numpy()\n",
    "\n",
    "# labels is now a NumPy array with the same number of rows as eeg_data, containing 1 for microwaking events and 0 otherwise"
   ],
   "id": "9479a0d495f14366",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels_df",
   "id": "fe0f88cc477e7816",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "labels_df['Microwaking'].value_counts()",
   "id": "2811f6e6b5ee220f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = eeg_data.reshape((1, eeg_data.shape[1], 1))\n",
    "y = labels.reshape((1, labels.shape[0], 1))\n",
    "timesteps, num_features = X.shape[1], 1\n",
    "#timesteps, num_features\n",
    "\n",
    "y2 = np.reshape(y, (y.shape[1], 1))\n",
    "X2 = np.reshape(X, (X.shape[1], X.shape[2]))\n",
    "y2 = y2.reshape((1, y2.shape[0], 1))\n",
    "\n",
    "X_reshaped = X.reshape((X.shape[1], 1))\n",
    "y_reshaped = y2.reshape((X.shape[1], 1))\n",
    "X = X_reshaped\n",
    "y = y_reshaped\n",
    "\n",
    "X.shape, y.shape\n"
   ],
   "id": "66f3521bc58447a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "timesteps, num_features",
   "id": "3a3ef92ee5e6c0f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "9318a5eb81273db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Verify if TensorFlow is using GPU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "id": "b350fa7ec4888d5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ],
   "id": "28d5b50ec9d7c227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Set the mixed precision policy\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ],
   "id": "d3fe8b62e30a3ce0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tf.get_logger().setLevel(logging.WARNING)\n",
    "tf.debugging.set_log_device_placement(False)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ],
   "id": "b25a77ec97106700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "X.shape, y.shape",
   "id": "e428c7eee1d024a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "de1d6bd600807493",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b3ebe01b72ee6e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def high_recall_loss(y_true, y_pred):\n",
    "    \"\"\"Custom loss function that heavily penalizes false negatives\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float16)\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    # Standard binary crossentropy\n",
    "    bce = y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n",
    "\n",
    "    # Additional penalty for false negatives\n",
    "    false_negative_penalty = 10.0 * y_true * K.log(y_pred)\n",
    "\n",
    "    return -K.mean(bce + false_negative_penalty, axis=-1)\n",
    "\n",
    "# Assuming X and y are your full dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(364479, 1)),\n",
    "    Conv1D(32, kernel_size=120, activation='relu', padding='same', name=\"Conv1D_1\"),\n",
    "    Conv1D(64, kernel_size=60, activation='relu', padding='same', name=\"Conv1D_2\"),\n",
    "    Conv1D(64, kernel_size=30, activation='relu', padding='same', name=\"Conv1D_3\"),\n",
    "    Conv1D(32, kernel_size=15, activation='relu', padding='same', name=\"Conv1D_4\"),\n",
    "    TimeDistributed(Dense(1, activation='sigmoid'))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=high_recall_loss,\n",
    "              metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"tensorboard in \" + log_dir)\n",
    "tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "class LogToStdout(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}, val_loss = {logs['val_loss']:.4f}, val_accuracy = {logs['val_accuracy']:.4f}\")\n",
    "\n",
    "# Instantiate the callback\n",
    "log_to_stdout = LogToStdout()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, tensorboard, log_to_stdout]\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Trained in: {elapsed_time:.6f} seconds\")"
   ],
   "id": "9900cc6add4df3cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# print(classification_report(y_val, adjusted_predictions))\n",
    "# print(confusion_matrix(y_val, adjusted_predictions))\n",
    "# \n",
    "# # Plot ROC curve to help choose optimal threshold\n",
    "# from sklearn.metrics import roc_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# fpr, tpr, thresholds = roc_curve(y_val.flatten(), predictions.flatten())\n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# plt.show()"
   ],
   "id": "61c214b8be082228",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# y_val.shape, predictions.shape",
   "id": "91ca313b7d83b43a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming predictions and y_val are your model outputs and validation labels\n",
    "#predictions = model.predict(X_val)\n",
    "predictions = model.predict(X_train)"
   ],
   "id": "e5e5fb33370fbd4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "# Ensure y_val is 2D and binary\n",
    "y_val_reshaped = y_val.reshape(y_val.shape[0], -1)\n",
    "y_val_binary = (y_val_reshaped > 0.5).astype(int)\n",
    "\n",
    "# Reshape predictions to match y_val\n",
    "adjusted_predictions = np.nan_to_num(predictions, nan=-1)\n",
    "predictions_reshaped = adjusted_predictions.reshape(adjusted_predictions.shape[0], -1)\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, thresholds = roc_curve(y_val_binary.ravel(), predictions_reshaped.ravel())\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# Function to compute metrics at different thresholds\n",
    "def compute_metrics_at_threshold(y_true, y_pred, threshold):\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    report = classification_report(y_true, y_pred_binary, output_dict=True)\n",
    "    return report['1']['precision'], report['1']['recall'], report['1']['f1-score']\n",
    "\n",
    "# Compute metrics at different thresholds\n",
    "thresholds_to_try = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "for threshold in thresholds_to_try:\n",
    "    precision, recall, f1 = compute_metrics_at_threshold(y_val_binary, predictions_reshaped, threshold)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "# Choose a threshold (this is an example, you should choose based on your requirements)\n",
    "chosen_threshold = 0.3\n",
    "adjusted_predictions = (predictions_reshaped > chosen_threshold).astype(int)\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "cr = classification_report(y_val_binary, adjusted_predictions)\n",
    "print(cr)\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_val_binary, adjusted_predictions)\n",
    "print(cm)"
   ],
   "id": "f1a506f9fd71e32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "cm",
   "id": "bf8ea7d4f2cced3a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "\n",
    "# Convert confusion matrix to a DataFrame with row/column labels\n",
    "cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'],\n",
    "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "cm_df"
   ],
   "id": "6e512cf125bf31e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.arange(len(predictions_reshaped))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x, predictions_reshaped)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Line Plot of Predictions')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Prediction Value')\n",
    "\n",
    "# Set y-axis limits\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Add a grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "id": "a1874817f8419784",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# predictions.shape, X_val.shape",
   "id": "5db5fe2c33380709",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "# import numpy as np\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense, Dropout, TimeDistributed, Input, GRU, GlobalAveragePooling1D\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Split the dataset into training and testing sets\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# \n",
    "# \n",
    "# # Build the LSTM-based sequence labeling model\n",
    "# # model = Sequential()\n",
    "# # \n",
    "# # model.add(Input(shape=(timesteps, num_features)))\n",
    "# # \n",
    "# # # Extract features with a CNN that we will feed to the LSTM\n",
    "# # model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "# # model.add(MaxPooling1D(pool_size=2))\n",
    "# # \n",
    "# # # GRU layer with return_sequences=True since we need a prediction for each time step\n",
    "# # model.add(GRU(units=64, return_sequences=True))\n",
    "# # \n",
    "# # # Dropout layer to avoid overfitting\n",
    "# # model.add(Dropout(0.2))\n",
    "# # \n",
    "# # # TimeDistributed Dense layer to predict a label for each time step (binary classification)\n",
    "# # model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
    "# # \n",
    "# # # Compile the model with binary crossentropy for binary classification\n",
    "# # model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# \n",
    "# model = Sequential([\n",
    "#     Input(shape=(X_reshaped.shape[0], X_reshaped.shape[1])),\n",
    "#     Conv1D(32, kernel_size=60, activation='relu', padding='same'),\n",
    "#     Conv1D(64, kernel_size=15, activation='relu', padding='same'),\n",
    "#     Conv1D(64, kernel_size=5, activation='relu', padding='same'),\n",
    "#     Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "#     TimeDistributed(Dense(1, activation='sigmoid'))\n",
    "# ])\n",
    "# \n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='binary_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "# \n",
    "# \n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "# tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "# \n",
    "# class LogToStdout(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         print(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}, val_loss = {logs['val_loss']:.4f}, val_accuracy = {logs['val_accuracy']:.4f}\")\n",
    "# \n",
    "# # Instantiate the callback\n",
    "# log_to_stdout = LogToStdout()\n",
    "# \n",
    "# # Train the model\n",
    "# history = model.fit(X_reshaped, y_reshaped, epochs=10, batch_size=32, callbacks=[early_stopping, tensorboard, log_to_stdout], validation_split=0.2)\n"
   ],
   "id": "d73c1be2bb90d9f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# probabilities = model.predict(X)",
   "id": "51b2ceb13ec8147a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# probabilities",
   "id": "ca69403b03524bbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save('./microwakings1.h5')\n",
   "id": "ee51488d2bf406c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# \n",
    "# # Flatten the arrays\n",
    "# probabilities_flat = probabilities.flatten()\n",
    "# y_reshaped_flat = y_reshaped.flatten()\n",
    "# \n",
    "# # Convert probabilities to binary predictions\n",
    "# predictions = (probabilities_flat > 0.5).astype(int)\n",
    "# \n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_reshaped_flat, predictions)\n",
    "# print(f'Accuracy: {accuracy:.4f}')"
   ],
   "id": "411f861b3d790dbb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Ensure probabilities is reshaped to match the number of samples\n",
    "# probabilities_channel = probabilities.flatten().reshape(1, -1)\n",
    "# \n",
    "# # Find the indexes of non-finite values\n",
    "# non_finite_indexes = np.where(~np.isfinite(probabilities_channel))[1]\n",
    "# \n",
    "# # Get the non-finite values\n",
    "# non_finite_values = probabilities_channel[0, non_finite_indexes]\n",
    "# \n",
    "# # Print the indexes and values of non-finite values\n",
    "# for idx, value in zip(non_finite_indexes, non_finite_values):\n",
    "#     print(f\"Index: {idx}, Value: {value}\")"
   ],
   "id": "a74ac33600d046e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "probabilities = model.predict(X)",
   "id": "7b0fc4350f6c50d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "\n",
    "# Step 1: Extract the existing data from the resampled object\n",
    "existing_data = resampled.get_data()\n",
    "\n",
    "# Step 2: Add a new channel with the probabilities data\n",
    "# Ensure probabilities is reshaped to match the number of samples\n",
    "probabilities_channel = probabilities.flatten().reshape(1, -1)\n",
    "\n",
    "# Step 3: Identify non-finite values and replace them with -50\n",
    "non_finite_indexes = np.where(~np.isfinite(probabilities_channel))[1]\n",
    "probabilities_channel[0, non_finite_indexes] = -50\n",
    "\n",
    "# Step 4: Multiply all other values by 50\n",
    "probabilities_channel = np.where(probabilities_channel == -50, -50, probabilities_channel * 50)\n",
    "\n",
    "# Step 5: Concatenate the new channel to the existing data\n",
    "new_data = np.vstack([existing_data, probabilities_channel])\n",
    "\n",
    "# Step 6: Create a new Info object for the new channel\n",
    "new_info = mne.create_info(\n",
    "    ch_names=resampled.ch_names + ['probabilities'],\n",
    "    sfreq=resampled.info['sfreq'],\n",
    "    ch_types=resampled.get_channel_types() + ['misc']\n",
    ")\n",
    "\n",
    "# Step 7: Create a new RawArray with the updated data and info\n",
    "new_raw = mne.io.RawArray(new_data, new_info)\n",
    "\n",
    "# Step 8: Save the modified data to an EDF file\n",
    "mne.export.export_raw(input_file_without_ext + \".with_predictions.edf\", new_raw, overwrite=True)"
   ],
   "id": "712f56ef1dfd9c52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.max(probabilities_channel)",
   "id": "131513e016a49ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Flatten the predictions array\n",
    "# predictions_flat = predictions.flatten()\n",
    "# \n",
    "# # Find the indexes of non-zero values\n",
    "# non_zero_indexes = np.where(predictions_flat != 0)[0]\n",
    "# \n",
    "# # Get the non-zero values\n",
    "# non_zero_values = predictions_flat[non_zero_indexes]\n",
    "# \n",
    "# # Print the indexes and values of non-zero values\n",
    "# for idx, value in zip(non_zero_indexes, non_zero_values):\n",
    "#     print(f\"Index: {idx}, Value: {value}\")"
   ],
   "id": "9fb3bc0db694bf8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Flatten the predictions array\n",
    "# predictions_flat = predictions.flatten()\n",
    "# \n",
    "# # Find the indexes of non-zero values\n",
    "# non_zero_indexes = np.where(predictions_flat != 0)[0]\n",
    "# \n",
    "# # Print the total count of non-zero values\n",
    "# print(f\"Total count of non-zero values in predictions: {len(non_zero_indexes)}\")\n",
    "# \n",
    "# # Flatten the y_reshaped array\n",
    "# y_reshaped_flat = y_reshaped.flatten()\n",
    "# \n",
    "# # Find the indexes of non-zero values\n",
    "# non_zero_indexes_y = np.where(y_reshaped_flat != 0)[0]\n",
    "# \n",
    "# # Print the total count of non-zero values\n",
    "# print(f\"Total count of non-zero values in y_reshaped: {len(non_zero_indexes_y)}\")"
   ],
   "id": "2f39d6ecd10c838c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a0171ee1a9b7458d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Replace NaN values in y_reshaped with -1\n",
    "# y_reshaped_2 = np.nan_to_num(y_reshaped, nan=-1)\n",
    "# adjusted_predictions_2 = np.nan_to_num(adjusted_predictions, nan=-1)\n",
    "# \n",
    "# print(classification_report(y_reshaped_2, adjusted_predictions_2))\n"
   ],
   "id": "26f1af81e3560401",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# print(confusion_matrix(y_reshaped_2, adjusted_predictions_2))\n",
   "id": "f0e472d0b95848c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# pd.DataFrame(adjusted_predictions_2.reshape(-1, 1)).value_counts()",
   "id": "f3a9e91465e9c58e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ddb6e44bd57466a5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
