{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Microwakings Model Using Single File\n",
    "Use a trained model (MicrowakingsTrainingMulti1) to detect microwakings, with a single file.\n"
   ],
   "id": "50381d01370cd08f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:15:40.898305Z",
     "start_time": "2024-09-13T07:15:40.833750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "log = lambda msg: logging.info(msg)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ],
   "id": "4e1134153c02b225",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load model",
   "id": "d32a3142bafd6e27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T07:15:50.310267Z",
     "start_time": "2024-09-13T07:15:41.835462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import microwakings1\n",
    "\n",
    "# Load the saved model\n",
    "model = microwakings1.load_model()\n",
    "\n",
    "# Verify the model is loaded by printing its summary\n",
    "model.summary()"
   ],
   "id": "14ff1ec87e81b7a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Conv1D_1 (Conv1D)           (None, None, 32)          3872      \n",
      "                                                                 \n",
      " Conv1D_2 (Conv1D)           (None, None, 64)          122944    \n",
      "                                                                 \n",
      " Conv1D_3 (Conv1D)           (None, None, 64)          122944    \n",
      "                                                                 \n",
      " Conv1D_4 (Conv1D)           (None, None, 32)          30752     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 128)         82432     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 128)         0         \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 1)          129       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 363,073\n",
      "Trainable params: 363,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load data",
   "id": "8329a7b6166610f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "dac3d377d7045c85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-12T11:58:13.381813Z",
     "start_time": "2024-09-12T11:58:13.242582Z"
    }
   },
   "cell_type": "code",
   "source": "input_dir = \"C:\\\\dev\\\\play\\\\brainwave-data\"\n",
   "id": "78323911b7120f8f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\dev\\\\play\\\\brainwave-processor')\n",
    "\n",
    "import run_yasa\n",
    "import run_feature_pipeline\n",
    "from models.microwakings_1.microwakings1 import PerFile\n",
    "import microwakings1 as helpers\n",
    "import mne\n",
    "\n",
    "out_files: list[PerFile] = []\n",
    "errors = []\n",
    "\n",
    "for root, dirs, files in os.walk(input_dir):\n",
    "    for dir_name in dirs:\n",
    "        input_file = os.path.join(root, dir_name, \"raw.fif\")\n",
    "        try:\n",
    "            log(\"Processing file: \" + input_file)\n",
    "            if os.path.exists(input_file):\n",
    "                input_file_without_ext = os.path.splitext(input_file)[0]\n",
    "                yasa_df = run_feature_pipeline.cached_pipeline(log, input_file)\n",
    "                mne_raw = mne.io.read_raw_fif(input_file, preload=True)\n",
    "                mne_filtered = run_yasa.get_filtered_and_scaled_data(mne_raw)\n",
    "                out_files.append(PerFile(input_file, None, mne_filtered, yasa_df, input_file_without_ext))\n",
    "        except Exception as e:\n",
    "            log(\"Error processing file: \" + input_file)\n",
    "            errors.append(\"Error processing file: \" + input_file + \" - \" + str(e))\n",
    "            log(e)\n",
    "\n",
    "for err in errors:\n",
    "    log(err)\n"
   ],
   "id": "1ada5d951c32e37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "errors",
   "id": "4b5b7621ca1fe1c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare model data",
   "id": "94818507543250fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import microwakings1\n",
    "\n",
    "for file in out_files:\n",
    "    df = file.prepare_model_data(resample_rate, True)\n",
    "    loc = grouped['DayOrNightOf'] == file.day_or_night_of\n",
    "    microwaking_counts = df['Microwaking'].value_counts()\n",
    "    grouped.loc[loc, 'MicrowakingSamples'] = microwaking_counts.get(1, 0)\n",
    "    grouped.loc[loc, 'OtherSamples'] = microwaking_counts.get(0, 0)\n",
    "\n",
    "grouped['TotalSamples'] = grouped['MicrowakingSamples'] + grouped['OtherSamples']\n",
    "grouped['MicrowakingSamplesPer'] = grouped['MicrowakingSamples'] / grouped['TotalSamples']\n",
    "grouped['TotalMins'] = grouped['TotalSamples'] / microwakings1.RESAMPLING_RATE / 60\n",
    "grouped['TotalHours'] = grouped['TotalMins'] / 60\n",
    "\n",
    "grouped"
   ],
   "id": "b7aa1582717be8c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train model",
   "id": "9318a5eb81273db2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "import gc\n",
    "\n",
    "# Reset the default graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Clear the Keras session\n",
    "K.clear_session()\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Limit GPU memory to 4GB (adjust as needed)\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\n",
    "        )\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "# Limit CPU memory growth\n",
    "# tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('CPU')[0], True)\n",
    "\n",
    "# Or set a specific memory limit (e.g., 4GB)\n",
    "# tf.config.experimental.set_virtual_device_configuration(\n",
    "#     tf.config.experimental.list_physical_devices('CPU')[0],\n",
    "#     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]\n",
    "# )"
   ],
   "id": "49ce19e49b1c12ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "id": "b350fa7ec4888d5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from tensorflow.keras import mixed_precision\n",
    "# \n",
    "# # Set the mixed precision policy\n",
    "# policy = mixed_precision.Policy('float32')\n",
    "# mixed_precision.set_global_policy(policy)"
   ],
   "id": "d3fe8b62e30a3ce0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# tf.get_logger().setLevel(logging.WARNING)\n",
    "# tf.debugging.set_log_device_placement(False)\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ],
   "id": "b25a77ec97106700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# \n",
    "# # Compute class weights - way less microwakings (1) than not (0) so they should get a much higher weighting\n",
    "# all_y = np.concatenate([file.y for file in out_files])\n",
    "# classes = np.array([0, 1])\n",
    "# class_weights = compute_class_weight('balanced', classes=classes, y=all_y)\n",
    "# # class_weight_tensor = tf.constant([class_weights[0], class_weights[1]], dtype=tf.float16)\n",
    "# class_weight_tensor = tf.constant([class_weights[0], class_weights[1]])\n",
    "# class_weight_dict = {int(classes[i]): float(class_weight_tensor.numpy()[i]) for i in range(len(classes))}\n",
    "# assert class_weight_dict[1] > class_weight_dict[0]\n",
    "# class_weight_dict"
   ],
   "id": "b486c75bcc76efe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, TimeDistributed, BatchNormalization, LSTM, GRU, Dropout, AveragePooling1D, UpSampling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "class DaySequence(Sequence):\n",
    "    def __init__(self, out_files):\n",
    "        self.out_files = out_files\n",
    "\n",
    "    # Batch size\n",
    "    def __len__(self):\n",
    "        return len(self.out_files)\n",
    "\n",
    "    # Get batch\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.out_files[idx]\n",
    "        X = file.X.reshape(-1, 1) #.astype('float16')  # Convert to float16\n",
    "        y = file.y.reshape(-1, 1) #.astype('float16')  # Convert to float16\n",
    "        X_out = X # np.expand_dims(X, axis=0)\n",
    "        y_out = y # np.expand_dims(y, axis=0)\n",
    "        # X = file.X.reshape(-1, 1)\n",
    "        # y = file.y.reshape(-1, 1)\n",
    "        # X_out = np.expand_dims(X, axis=0)\n",
    "        #print(f\"file = {file.day_or_night_of} X_out.shape = {X_out.shape}, y.shape = {y_out.shape}\")\n",
    "        return X_out, y_out\n",
    "    \n",
    "day_sequence = DaySequence(out_files)\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(None, 1)),\n",
    "\n",
    "    # Try to detect features\n",
    "    Conv1D(32, kernel_size=120, activation='relu', padding='same', name=\"Conv1D_1\"),\n",
    "    Conv1D(64, kernel_size=60, activation='relu', padding='same', name=\"Conv1D_2\"),\n",
    "    Conv1D(64, kernel_size=30, activation='relu', padding='same', name=\"Conv1D_3\"),\n",
    "    Conv1D(32, kernel_size=15, activation='relu', padding='same', name=\"Conv1D_4\"),\n",
    "    \n",
    "    # Give it some memory\n",
    "    LSTM(units=128, return_sequences=True),\n",
    "\n",
    "    # Temporal pooling to try to encourage it to output the same throughout the waking\n",
    "    AveragePooling1D(pool_size=10),\n",
    "    Conv1D(32, kernel_size=3, activation='relu', padding='same'),\n",
    "    UpSampling1D(size=10),\n",
    "\n",
    "    # Dropout(0.2),\n",
    "    # LSTM(units=64, return_sequences=True),  \n",
    "    TimeDistributed(Dense(1, activation='sigmoid'))\n",
    "])\n",
    "\n",
    "\n",
    "# 3/3 [==============================] - 15s 2s/step - loss: 4.1520 - cross entropy: 0.6935 - Brier score: 0.2502 - tp: 11283.0000 - fp: 531622.0000 - tn: 144562.0000 - fn: 3869.0000 - accuracy: 0.2254 - precision: 0.0208 - recall: 0.7447 - auc: 0.5248 - prc: 0.0652\n",
    "# 3/3 [==============================] - 0s 135ms/step - loss: 1.2523 - cross entropy: 1.4580 - Brier score: 0.5408 - tp: 15152.0000 - fp: 676184.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.0219 - precision: 0.0219 - recall: 1.0000 - auc: 0.8694 - prc: 0.4497\n",
    "\n",
    "METRICS = [\n",
    "    #  difference between the predicted probabilities and the actual class labels.\n",
    "    keras.metrics.BinaryCrossentropy(name='cross entropy'),\n",
    "    #  The Brier score measures the accuracy of probabilistic predictions. It ranges from 0 to 1, with lower values indicating better accuracy\n",
    "    keras.metrics.MeanSquaredError(name='Brier score'),\n",
    "    # The number of correctly predicted positive samples. \n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    # The number of negative samples incorrectly predicted as positive.\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    #  The number of correctly predicted negative samples.\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    # The number of positive samples incorrectly predicted as negative.\n",
    "    keras.metrics.FalseNegatives(name='fn'),\n",
    "    #  The overall accuracy of the model, calculated as (tp + tn) / (tp + tn + fp + fn). Here, it is 98.17%.\n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    # Precision is the ratio of true positives to the sum of true positives and false positives (tp / (tp + fp)). It indicates how many of the predicted positives are actually positive.\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    # Recall is the ratio of true positives to the sum of true positives and false negatives (tp / (tp + fn)). It indicates how many of the actual positives are correctly predicted.\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    # The Area Under the ROC Curve (AUC) measures the model's ability to distinguish between classes. It ranges from 0 to 1, with higher values indicating better performance.\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "    # The Area Under the Precision-Recall Curve (PRC) measures the trade-off between precision and recall for different threshold values. Higher values indicate better performance. \n",
    "    keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "# Design a loss function that penalizes rapid changes in the output, encouraging longer continuous periods.\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Binary cross-entropy\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    # Penalty for rapid changes\n",
    "    diff = K.abs(y_pred[:, 1:] - y_pred[:, :-1])\n",
    "    change_penalty = K.mean(diff)\n",
    "\n",
    "    return bce + 0.1 * change_penalty\n",
    "\n",
    "# def focal_loss(gamma=2., alpha=0.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         log(f\"focal_loss_fixed y_true = {y_true.shape} y_pred = {y_pred.shape}\")\n",
    "#         epsilon = K.epsilon()\n",
    "#         y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "#         y_true = K.cast(y_true, K.floatx())\n",
    "#         alpha_t = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
    "#         p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
    "#         fl = - alpha_t * K.pow((1 - p_t), gamma) * K.log(p_t)\n",
    "#         log(f\"focal_loss_fixed y_true = {y_true.shape} y_pred = {y_pred.shape} fl = {fl.shape} K.mean(fl) = {K.mean(fl)}\")\n",
    "#         return K.mean(fl, axis=-1)\n",
    "#     return focal_loss_fixed\n",
    "# \n",
    "# def high_recall_loss(y_true, y_pred):\n",
    "#     \"\"\"Custom loss function that heavily penalizes false negatives\"\"\"\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "#     y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "# \n",
    "#     # Standard binary crossentropy\n",
    "#     bce = y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)\n",
    "# \n",
    "#     # Additional penalty for false negatives\n",
    "#     false_negative_penalty = 10.0 * y_true * K.log(y_pred)\n",
    "# \n",
    "#     return -K.mean(bce + false_negative_penalty, axis=-1)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              # loss=high_recall_loss,\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "\n",
    "              # loss=keras.losses.BinaryFocalCrossentropy(apply_class_balancing = True),\n",
    "              # loss=focal_loss(gamma=2., alpha=0.25),\n",
    "\n",
    "              # loss=keras.losses.BinaryCrossentropy(),\n",
    "              # loss='binary_focal_crossentropy',\n",
    "              metrics=METRICS\n",
    "              #metrics=['accuracy', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]\n",
    "              )\n",
    "# \n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# \n",
    "# class LogToStdout(tf.keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         log(f\"Epoch end {epoch + 1} {logs}\")\n",
    "#         # print(f\"Epoch {epoch + 1}: loss = {logs['loss']:.4f}, accuracy = {logs['accuracy']:.4f}, recall = {logs['recall']:.4f}, precision = {logs['precision']:.4f}\")\n",
    "# \n",
    "# log_to_stdout = LogToStdout()\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=0.0001)\n",
    "# early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "# \n",
    "# start_time = time.time()\n",
    "# \n",
    "history = model.fit(\n",
    "    day_sequence,\n",
    "    epochs=100,\n",
    "    # callbacks=[log_to_stdout, tensorboard],\n",
    "    # callbacks=[tensorboard, log_to_stdout, reduce_lr, early_stopping],\n",
    "    # class_weight=class_weight_dict\n",
    ")\n",
    "\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Trained in: {elapsed_time:.6f} seconds\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ],
   "id": "9900cc6add4df3cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "d5751e666a4b72eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# \n",
    "# # Evaluation\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# \n",
    "# for file in out_files:\n",
    "#     X = file.X.reshape(1, -1, 1)  # Reshape to (1, timesteps, 1)\n",
    "#     y_true.extend(file.y)\n",
    "#     y_pred_file = model.predict(X)\n",
    "#     y_pred.extend(y_pred_file.flatten())\n",
    "# \n",
    "# y_true = np.array(y_true)\n",
    "# y_pred = np.array(y_pred)\n",
    "# \n",
    "# # Apply the same threshold\n",
    "# threshold = 0.5\n",
    "# y_pred_binary = (y_pred > threshold).astype(int)\n",
    "# \n",
    "# # Recompute the confusion matrix\n",
    "# cm = confusion_matrix(y_true, y_pred_binary)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(cm)\n",
    "# \n",
    "# # Classification Report\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_true, y_pred_binary))"
   ],
   "id": "cb75054365621bdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "# \n",
    "# class DaySequencePred(Sequence):\n",
    "#     def __init__(self, out_files):\n",
    "#         self.out_files = out_files\n",
    "# \n",
    "#     # Batch size\n",
    "#     def __len__(self):\n",
    "#         return len(self.out_files)\n",
    "# \n",
    "#     # Get batch\n",
    "#     def __getitem__(self, idx):\n",
    "#         file = self.out_files[idx]\n",
    "#         X = file.X.reshape(-1, 1) #.astype('float16')  # Convert to float16\n",
    "#         y = file.y.reshape(-1, 1) #.astype('float16')  # Convert to float16\n",
    "#         X_out = X # np.expand_dims(X, axis=0)\n",
    "#         y_out = y # np.expand_dims(y, axis=0)\n",
    "#         # X = file.X.reshape(-1, 1)\n",
    "#         # y = file.y.reshape(-1, 1)\n",
    "#         # X_out = np.expand_dims(X, axis=0)\n",
    "#         #print(f\"file = {file.day_or_night_of} X_out.shape = {X_out.shape}, y.shape = {y_out.shape}\")\n",
    "#         return X_out, y_out\n",
    "# \n",
    "# day_sequence_pred = DaySequencePred([out_files[0]])\n",
    "# \n",
    "# \n",
    "# y_true_flat = np.concatenate([file.y for file in out_files])\n",
    "# y_pred_proba = model.predict(day_sequence_pred).flatten()\n",
    "# y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "# \n",
    "# manual_accuracy = accuracy_score(y_true_flat, y_pred_binary)\n",
    "# manual_precision = precision_score(y_true_flat, y_pred_binary)\n",
    "# manual_recall = recall_score(y_true_flat, y_pred_binary)\n",
    "# manual_auc = roc_auc_score(y_true_flat, y_pred_proba)\n",
    "# \n",
    "# print(\"Manually calculated metrics:\")\n",
    "# print(f\"Accuracy: {manual_accuracy:.4f}\")\n",
    "# print(f\"Precision: {manual_precision:.4f}\")\n",
    "# print(f\"Recall: {manual_recall:.4f}\")\n",
    "# print(f\"AUC: {manual_auc:.4f}\")\n",
    "# \n",
    "# cm = confusion_matrix(y_true_flat, y_pred_binary)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(cm)"
   ],
   "id": "14acf242fde68550",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from tensorflow.keras.utils import Sequence\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "# \n",
    "# # Function to evaluate model on given files\n",
    "# def evaluate_model(model, files):\n",
    "#     day_sequence_pred = FlexibleDaySequence(files)\n",
    "# \n",
    "#     # Get true labels\n",
    "#     y_true_flat = np.concatenate([file.y for file in files])\n",
    "# \n",
    "#     # Make predictions\n",
    "#     y_pred_proba = model.predict(day_sequence_pred).flatten()\n",
    "#     y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "# \n",
    "#     # Calculate metrics\n",
    "#     manual_accuracy = accuracy_score(y_true_flat, y_pred_binary)\n",
    "#     manual_precision = precision_score(y_true_flat, y_pred_binary)\n",
    "#     manual_recall = recall_score(y_true_flat, y_pred_binary)\n",
    "#     manual_auc = roc_auc_score(y_true_flat, y_pred_proba)\n",
    "# \n",
    "#     print(\"Manually calculated metrics:\")\n",
    "#     print(f\"Accuracy: {manual_accuracy:.4f}\")\n",
    "#     print(f\"Precision: {manual_precision:.4f}\")\n",
    "#     print(f\"Recall: {manual_recall:.4f}\")\n",
    "#     print(f\"AUC: {manual_auc:.4f}\")\n",
    "# \n",
    "#     cm = confusion_matrix(y_true_flat, y_pred_binary)\n",
    "#     print(\"\\nConfusion Matrix:\")\n",
    "#     print(cm)\n",
    "# \n",
    "#     # Additional analysis\n",
    "#     print(\"\\nPrediction Statistics:\")\n",
    "#     print(f\"Min prediction: {y_pred_proba.min():.4f}\")\n",
    "#     print(f\"Max prediction: {y_pred_proba.max():.4f}\")\n",
    "#     print(f\"Mean prediction: {y_pred_proba.mean():.4f}\")\n",
    "#     print(f\"Median prediction: {np.median(y_pred_proba):.4f}\")\n",
    "# \n",
    "#     # Distribution of predictions\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.hist(y_pred_proba, bins=50)\n",
    "#     plt.title(f\"Distribution of Predictions for {len(files)} file(s)\")\n",
    "#     plt.xlabel(\"Prediction Value\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.show()\n",
    "# \n",
    "# # Evaluate on single file\n",
    "# print(\"Evaluating single file:\")\n",
    "# evaluate_model(model, [out_files[1]])\n",
    "# \n",
    "# # Evaluate on all files\n",
    "# # print(\"\\nEvaluating all files:\")\n",
    "# # evaluate_model(model, out_files)"
   ],
   "id": "fdba132b5b7dcbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "# import tensorflow as tf\n",
    "# \n",
    "# # Process only out_files[1]\n",
    "# file = out_files[1]\n",
    "# \n",
    "# # Prepare the input data\n",
    "# X = file.X.reshape(1, -1, 1)  # Reshape to (1, timesteps, 1)\n",
    "# y_true = file.y\n",
    "# \n",
    "# # Make predictions\n",
    "# y_pred_proba = model.predict(X).flatten()\n",
    "# y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "# \n",
    "# # Calculate metrics\n",
    "# accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "# precision = precision_score(y_true, y_pred_binary)\n",
    "# recall = recall_score(y_true, y_pred_binary)\n",
    "# auc = roc_auc_score(y_true, y_pred_proba)\n",
    "# \n",
    "# # Print results\n",
    "# print(f\"File: {file.day_or_night_of}\")\n",
    "# print(f\"X shape: {X.shape}\")\n",
    "# print(f\"y_true shape: {y_true.shape}\")\n",
    "# print(f\"y_pred_proba shape: {y_pred_proba.shape}\")\n",
    "# \n",
    "# print(\"\\nMetrics:\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"Precision: {precision:.4f}\")\n",
    "# print(f\"Recall: {recall:.4f}\")\n",
    "# print(f\"AUC: {auc:.4f}\")\n",
    "# \n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_true, y_pred_binary))\n",
    "# \n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_true, y_pred_binary))\n",
    "# \n",
    "# # Additional analysis of predictions\n",
    "# print(\"\\nPrediction Statistics:\")\n",
    "# print(f\"Min prediction: {y_pred_proba.min():.4f}\")\n",
    "# print(f\"Max prediction: {y_pred_proba.max():.4f}\")\n",
    "# print(f\"Mean prediction: {y_pred_proba.mean():.4f}\")\n",
    "# print(f\"Median prediction: {np.median(y_pred_proba):.4f}\")\n",
    "# \n",
    "# # Distribution of predictions\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.hist(y_pred_proba, bins=50)\n",
    "# plt.title(f\"Distribution of Predictions for {file.day_or_night_of}\")\n",
    "# plt.xlabel(\"Prediction Value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.show()\n",
    "# \n",
    "# # Check predictions against different thresholds\n",
    "# thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "# for thresh in thresholds:\n",
    "#     above_thresh = (y_pred_proba > thresh).sum()\n",
    "#     print(f\"Predictions above {thresh}: {above_thresh} ({above_thresh/len(y_pred_proba):.2%})\")\n",
    "# \n",
    "# # Compare with Keras metrics\n",
    "# keras_metrics = model.evaluate(X, y_true, verbose=0)\n",
    "# metric_names = ['loss', 'cross entropy', 'Brier score', 'tp', 'fp', 'tn', 'fn', 'accuracy', 'precision', 'recall', 'auc', 'prc']\n",
    "# \n",
    "# print(\"\\nKeras Metrics:\")\n",
    "# for name, value in zip(metric_names, keras_metrics):\n",
    "#     print(f\"{name}: {value:.4f}\")"
   ],
   "id": "6237f1e9e1fdd8a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# np.max(y_pred)",
   "id": "51b3dafe1cc54e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# \n",
    "# # Evaluation\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# \n",
    "# for file in out_files:\n",
    "#     X = file.X.reshape(1, -1, 1)  # Reshape to (1, timesteps, 1)\n",
    "#     y_true.extend(file.y)\n",
    "#     y_pred_file = model.predict(X)\n",
    "#     y_pred.extend(np.nan_to_num(y_pred_file, nan=-1).flatten())\n",
    "# \n",
    "# y_true = np.array(y_true)\n",
    "# y_pred = np.array(y_pred)\n",
    "# \n",
    "# # ROC Curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# \n",
    "# plt.figure()\n",
    "# plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()\n",
    "# \n",
    "# # Extract loss values from the history object\n",
    "# loss = history.history['loss']\n",
    "# \n",
    "# # Plot the loss values\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(loss, label='Training Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Time')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# \n",
    "# \n",
    "# \n",
    "# # Classification Report\n",
    "# y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_true, y_pred_binary))\n",
    "# \n",
    "# # Confusion Matrix\n",
    "# cm = confusion_matrix(y_true, y_pred_binary)\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(cm)\n",
    "# \n",
    "# labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "# \n",
    "# # Convert confusion matrix to a DataFrame with row/column labels\n",
    "# cm_df = pd.DataFrame(cm, index=['Actual Negative', 'Actual Positive'],\n",
    "#                      columns=['Predicted Negative', 'Predicted Positive'])\n",
    "# \n",
    "# display(cm_df)"
   ],
   "id": "f1a506f9fd71e32b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.save('./microwakings_multi1.h5')\n",
   "id": "ee51488d2bf406c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# f = out_files[0]\n",
    "# np.mean(f.eeg_data)"
   ],
   "id": "7fb9dc724a839ebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Example NumPy array\n",
    "# array = probabilities\n",
    "# \n",
    "# # Calculate the mean\n",
    "# mean_value = np.mean(array)\n",
    "# \n",
    "# # Calculate the standard deviation\n",
    "# stddev_value = np.std(array)\n",
    "# \n",
    "# # Calculate the 25th, 50th (median), and 75th percentiles\n",
    "# percentile_25 = np.percentile(array, 25)\n",
    "# percentile_50 = np.percentile(array, 50)\n",
    "# percentile_75 = np.percentile(array, 75)\n",
    "# \n",
    "# # Print the results\n",
    "# print(\"Mean of the array:\", mean_value)\n",
    "# print(\"Standard Deviation of the array:\", stddev_value)\n",
    "# print(\"25th Percentile of the array:\", percentile_25)\n",
    "# print(\"50th Percentile (Median) of the array:\", percentile_50)\n",
    "# print(\"75th Percentile of the array:\", percentile_75)"
   ],
   "id": "322ce2892355e70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# \n",
    "# for file in out_files:\n",
    "#     X_reshaped = file.X.reshape(1, -1, 1)\n",
    "#     pred = model.predict(X_reshaped)\n",
    "#     pred_flatten = pred.flatten()\n",
    "# \n",
    "#     # Calculate spread of values\n",
    "#     min_value = np.min(pred_flatten)\n",
    "#     max_value = np.max(pred_flatten)\n",
    "#     mean_value = np.mean(pred_flatten)\n",
    "#     stddev_value = np.std(pred_flatten)\n",
    "#     percentile_25 = np.percentile(pred_flatten, 25)\n",
    "#     percentile_50 = np.percentile(pred_flatten, 50)\n",
    "#     percentile_75 = np.percentile(pred_flatten, 75)\n",
    "# \n",
    "#     # Calculate accuracy\n",
    "#     y_true = file.y\n",
    "#     y_pred_binary = (pred_flatten > 0.5).astype(int)\n",
    "#     accuracy = accuracy_score(y_true, y_pred_binary)\n",
    "# \n",
    "#     # Print results\n",
    "#     print(f\"File: {file.day_or_night_of}\")\n",
    "#     print(f\"Spread of values - Min: {min_value}, Max: {max_value}, Mean: {mean_value}, Stddev: {stddev_value}\")\n",
    "#     print(f\"Percentiles - 25th: {percentile_25}, 50th (Median): {percentile_50}, 75th: {percentile_75}\")\n",
    "#     print(f\"Accuracy: {accuracy}\")\n",
    "#     print()"
   ],
   "id": "598d5fb95210e950",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "class FlexibleDaySequence(Sequence):\n",
    "    def __init__(self, out_files):\n",
    "        self.out_files = out_files\n",
    "        self.total_samples = sum(file.X.shape[0] for file in out_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # We'll process all data in one batch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X_list = []\n",
    "        y_list = []\n",
    "        for file in self.out_files:\n",
    "            X_list.append(file.X.reshape(-1, 1))\n",
    "            y_list.append(file.y.reshape(-1, 1))\n",
    "        X_out = np.concatenate(X_list, axis=0)\n",
    "        y_out = np.concatenate(y_list, axis=0)\n",
    "        #return X_out, y_out\n",
    "        return X_out\n",
    "\n",
    "\n",
    "def predict_file(f):\n",
    "    display(f.input_file_without_ext)\n",
    "    \n",
    "    # Step 1: Extract the existing data from the resampled object\n",
    "    existing_data = f.eeg_data\n",
    "    display(existing_data.shape)\n",
    "    \n",
    "    # Step 2: Predict probabilities and ensure they match the number of samples in existing data\n",
    "    y_true_flat = f.y\n",
    "    day_sequence_pred = FlexibleDaySequence([f])\n",
    "    y_pred_proba = model.predict(day_sequence_pred).flatten()\n",
    "    y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
    "    # display(probabilities.shape)\n",
    "    num_samples = existing_data.shape[1]\n",
    "    # display(num_samples)\n",
    "    \n",
    "    manual_accuracy = accuracy_score(y_true_flat, y_pred_binary)\n",
    "    manual_precision = precision_score(y_true_flat, y_pred_binary)\n",
    "    manual_recall = recall_score(y_true_flat, y_pred_binary)\n",
    "    manual_auc = roc_auc_score(y_true_flat, y_pred_proba)\n",
    "    \n",
    "    print(\"Manually calculated metrics:\")\n",
    "    print(f\"Accuracy: {manual_accuracy:.4f}\")\n",
    "    print(f\"Precision: {manual_precision:.4f}\")\n",
    "    print(f\"Recall: {manual_recall:.4f}\")\n",
    "    print(f\"AUC: {manual_auc:.4f}\")\n",
    "    \n",
    "    cm = confusion_matrix(y_true_flat, y_pred_binary)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # Additional analysis\n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(f\"Min prediction: {y_pred_proba.min():.4f}\")\n",
    "    print(f\"Max prediction: {y_pred_proba.max():.4f}\")\n",
    "    print(f\"Mean prediction: {y_pred_proba.mean():.4f}\")\n",
    "    print(f\"Median prediction: {np.median(y_pred_proba):.4f}\")\n",
    "    \n",
    "    # Ensure probabilities have the same number of samples as existing data\n",
    "    # if probabilities.shape[0] != num_samples:\n",
    "    #     probabilities = np.resize(probabilities, (num_samples,))\n",
    "    \n",
    "    # Step 3: Add a new channel with the probabilities data\n",
    "    high = 400_000_000\n",
    "    low = 200_000_000\n",
    "    probabilities_channel = y_pred_proba.reshape(1, -1) * high\n",
    "    \n",
    "    # pred_flatten = probabilities_channel.flatten()\n",
    "    # min_value = np.min(pred_flatten)\n",
    "    # max_value = np.max(pred_flatten)\n",
    "    # mean_value = np.mean(pred_flatten)\n",
    "    # stddev_value = np.std(pred_flatten)\n",
    "    # percentile_25 = np.percentile(pred_flatten, 25)\n",
    "    # percentile_50 = np.percentile(pred_flatten, 50)\n",
    "    # percentile_75 = np.percentile(pred_flatten, 75)\n",
    "    # \n",
    "    # print(f\"Spread of values - Min: {min_value}, Max: {max_value}, Mean: {mean_value}, Stddev: {stddev_value}\")\n",
    "    # print(f\"Percentiles - 25th: {percentile_25}, 50th (Median): {percentile_50}, 75th: {percentile_75}\")\n",
    "    \n",
    "    # Step 4: Identify non-finite values and replace them with -50\n",
    "    # non_finite_indexes = np.where(~np.isfinite(probabilities_channel))[1]\n",
    "    # probabilities_channel[0, non_finite_indexes] = -50\n",
    "    \n",
    "    # Step 5: Multiply all other values by 50\n",
    "    # probabilities_channel = np.where(probabilities_channel == -50, -50, probabilities_channel * 50)\n",
    "    \n",
    "    print(f\"existing_data.shape: {existing_data.shape}\")\n",
    "    print(f\"probabilities_channel.shape: {probabilities_channel.shape}\")\n",
    "    \n",
    "    def smooth_predictions(predictions, window_length=11, polyorder=2, moving_avg_window=5):\n",
    "        # Apply Savitzky-Golay filter\n",
    "        smoothed = savgol_filter(predictions, window_length, polyorder)\n",
    "        # Apply moving average\n",
    "        smoothed = np.convolve(smoothed, np.ones(moving_avg_window)/moving_avg_window, mode='same')\n",
    "        return smoothed\n",
    "    \n",
    "    smoothed = smooth_predictions(probabilities_channel[0])\n",
    "\n",
    "    \n",
    "\n",
    "    def binary_split_with_values(predictions, threshold=low, high_value=high, low_value=0):\n",
    "        return np.where(predictions > threshold, high_value, low_value)\n",
    "\n",
    "    def fill_gaps_with_values(predictions, sample_rate, gap_duration=2, high_value=high, low_value=0):\n",
    "        gap_samples = gap_duration * sample_rate\n",
    "        filled_predictions = predictions.copy()\n",
    "    \n",
    "        zero_streak_start = None\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == low_value:\n",
    "                if zero_streak_start is None:\n",
    "                    zero_streak_start = i\n",
    "            else:\n",
    "                if zero_streak_start is not None:\n",
    "                    if i - zero_streak_start <= gap_samples:\n",
    "                        filled_predictions[zero_streak_start:i] = high_value\n",
    "                    zero_streak_start = None\n",
    "    \n",
    "        # Handle case where the streak goes till the end\n",
    "        if zero_streak_start is not None and len(predictions) - zero_streak_start <= gap_samples:\n",
    "            filled_predictions[zero_streak_start:] = high_value\n",
    "    \n",
    "        return filled_predictions\n",
    "\n",
    "    def remove_short_periods(predictions, sample_rate, period_duration=2, high_value=high, low_value=0):\n",
    "        period_samples = period_duration * sample_rate\n",
    "        cleaned_predictions = predictions.copy()\n",
    "    \n",
    "        high_streak_start = None\n",
    "        for i in range(len(predictions)):\n",
    "            if predictions[i] == high_value:\n",
    "                if high_streak_start is None:\n",
    "                    high_streak_start = i\n",
    "            else:\n",
    "                if high_streak_start is not None:\n",
    "                    if i - high_streak_start < period_samples:\n",
    "                        cleaned_predictions[high_streak_start:i] = low_value\n",
    "                    high_streak_start = None\n",
    "    \n",
    "        # Handle case where the streak goes till the end\n",
    "        if high_streak_start is not None and len(predictions) - high_streak_start < period_samples:\n",
    "            cleaned_predictions[high_streak_start:] = low_value\n",
    "    \n",
    "        return cleaned_predictions\n",
    "\n",
    "    def process_predictions(predictions, threshold=low, sample_rate=10, gap_duration=2, period_duration=2, high_value=high, low_value=0):\n",
    "        binary_predictions = binary_split_with_values(predictions, threshold, high_value, low_value)\n",
    "        cleaned_initial_predictions = remove_short_periods(binary_predictions, sample_rate, 0.5, high_value, low_value)\n",
    "        filled_predictions = fill_gaps_with_values(cleaned_initial_predictions, sample_rate, gap_duration, high_value, low_value)\n",
    "        cleaned_predictions = remove_short_periods(filled_predictions, sample_rate, period_duration, high_value, low_value)\n",
    "        return cleaned_predictions\n",
    "\n",
    "    binary_predictions = binary_split_with_values(probabilities_channel[0])\n",
    "    cleaned_initial_predictions = remove_short_periods(binary_predictions, 10, 0.5, high, 0)\n",
    "    filled_predictions = fill_gaps_with_values(cleaned_initial_predictions, 10, 2, high, 0)\n",
    "    cleaned_predictions = remove_short_periods(filled_predictions, 10, 2, high, 0)\n",
    "    processed_predictions = process_predictions(probabilities_channel[0])\n",
    "\n",
    "    processed_predictions_0_1 = np.where(processed_predictions > low, 1, 0)\n",
    "    \n",
    "    manual_accuracy = accuracy_score(y_true_flat, processed_predictions_0_1)\n",
    "    manual_precision = precision_score(y_true_flat, processed_predictions_0_1)\n",
    "    manual_recall = recall_score(y_true_flat, processed_predictions_0_1)\n",
    "    cm = confusion_matrix(y_true_flat, processed_predictions_0_1)\n",
    "\n",
    "    print(f\"After processing\")\n",
    "    print(f\"Accuracy: {manual_accuracy:.4f}\")\n",
    "    print(f\"Precision: {manual_precision:.4f}\")\n",
    "    print(f\"Recall: {manual_recall:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Step 6: Concatenate the new channel to the existing data\n",
    "    new_data = np.vstack([f.X, probabilities_channel, processed_predictions, binary_predictions, cleaned_initial_predictions, filled_predictions, cleaned_predictions])\n",
    "    \n",
    "    # Verify the shapes\n",
    "    print(f\"new_data.shape: {new_data.shape}\")\n",
    "    \n",
    "    # Step 7: Create a new Info object for the new channel\n",
    "    new_info = mne.create_info(\n",
    "        ch_names=[f.most_frequent_channel, 'final', 'raw', 'binary', 'cleaned1', 'filled', 'cleaned2'],\n",
    "        sfreq=10,\n",
    "        ch_types=['eeg', 'misc', 'misc', 'misc', 'misc', 'misc', 'misc']\n",
    "    )\n",
    "    \n",
    "    # Ensure the number of channels in new_data matches the number of channel names in new_info\n",
    "    assert new_data.shape[0] == len(new_info['ch_names'])\n",
    "    \n",
    "    new_data_scaled = new_data / 1_000_000\n",
    "    \n",
    "    # Step 8: Create a new RawArray with the updated data and info\n",
    "    new_raw = mne.io.RawArray(new_data_scaled, new_info)\n",
    "    \n",
    "    # Step 9: Save the modified data to an EDF file\n",
    "    mne.export.export_raw(f.input_file_without_ext + \".with_predictions.edf\", new_raw, overwrite=True)\n",
    "\n",
    "f = out_files[0]\n",
    "predict_file(f)"
   ],
   "id": "b2db41144e00c1bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Use a custom training loop for more control\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# #@tf.function\n",
    "# def train_step(epoch, x, y, class_weights):\n",
    "#     start_time = time.time()\n",
    "#     log(f\"Epoch {epoch} batch (day) starting\")\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         log(\"Making training pred\")\n",
    "#         y_pred = model(x, training=True)\n",
    "#         log(\"Getting loss\")\n",
    "#         loss = tf.keras.losses.binary_focal_crossentropy(y, y_pred)\n",
    "#         log(\"Got loss\")\n",
    "#         # Apply class weights\n",
    "#         sample_weights = tf.where(tf.equal(y, 1), class_weights[1], class_weights[0])\n",
    "#         log(\"A\")\n",
    "#         loss = tf.reduce_mean(loss * sample_weights)\n",
    "#         log(\"B\")\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n",
    "# \n",
    "#     log(\"C\")\n",
    "#     # log(str(gradients))\n",
    "#     # log(str(model.trainable_variables))\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     log(\"D\")\n",
    "#     # end_time = time.time()\n",
    "#     # elapsed_time = end_time - start_time\n",
    "#     #log(f\"Epoch {epoch} batch (day) trained in: {elapsed_time:.6f} seconds with loss = {loss:.6f}\")\n",
    "#     return loss\n",
    "# \n",
    "# # Training loop\n",
    "# for epoch in range(50):\n",
    "#     total_loss = 0\n",
    "#     log(\"0\")\n",
    "#     for x_batch, y_batch in day_sequence:\n",
    "#         log(\"X\")\n",
    "#         loss = train_step(epoch, x_batch, y_batch, class_weight_dict)\n",
    "#         log(\"E\")\n",
    "#         #log(f\"Epoch {epoch + 1} finished batch with loss: {loss}\")\n",
    "#         total_loss += loss\n",
    "#     log(\"F\")\n",
    "\n",
    "# log(f\"Epoch {epoch + 1}, Loss: {total_loss / len(day_sequence)}\")\n"
   ],
   "id": "1e4523d98e6e4b05",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
